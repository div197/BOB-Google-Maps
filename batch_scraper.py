#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=============================================================================
å¤§è§„æ¨¡æ‰¹é‡Google Mapsæ•°æ®çˆ¬å–å·¥å…· - å¢å¼ºç‰ˆ
=============================================================================

ä¸“ä¸ºå¤„ç†æ•°ä¸‡çº§åœ°ç‚¹æ•°æ®è®¾è®¡çš„é«˜å¯é æ€§æ‰¹é‡çˆ¬è™«å·¥å…·

ä¸»è¦ç‰¹æ€§ï¼š
- æ”¯æŒæ•°ä¸‡ä¸ªåœ°ç‚¹çš„æ‰¹é‡å¤„ç†
- å¼ºå¤§çš„æ–­ç‚¹æ¢å¤æœºåˆ¶ï¼Œæ”¯æŒæ„å¤–ä¸­æ–­åç»§ç»­å¤„ç†
- æ™ºèƒ½é‡è¯•æ¨¡å¼ï¼Œå¯é€‰æ‹©æ€§é‡è¯•å¤±è´¥çš„åœ°ç‚¹
- æŒ‰ parent_type/sub_type/place_id_åç§°.json ç»„ç»‡è¾“å‡ºæ–‡ä»¶
- å¤šçº¿ç¨‹å¹¶å‘å¤„ç†ï¼Œå†…å­˜ä¼˜åŒ–
- è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ªå’Œé”™è¯¯å¤„ç†
- å¤„ç†æ•°é‡é™åˆ¶ï¼Œä¾¿äºæµ‹è¯•å’Œåˆ†æ‰¹å¤„ç†

å¼‚å¸¸å¤„ç†æœºåˆ¶ï¼š
1. æ¯ä¸ªåœ°ç‚¹ç‹¬ç«‹å¤„ç†ï¼Œå•ç‚¹å¤±è´¥ä¸å½±å“æ•´ä½“
2. è‡ªåŠ¨é‡è¯•æœºåˆ¶ï¼Œç½‘ç»œè¶…æ—¶ç­‰ä¸´æ—¶é—®é¢˜å¯é‡è¯•
3. è¯¦ç»†é”™è¯¯åˆ†ç±»å’Œè®°å½•ï¼Œä¾¿äºé—®é¢˜è¯Šæ–­
4. å®æ—¶è¿›åº¦ä¿å­˜ï¼Œæ¯10ä¸ªåœ°ç‚¹è‡ªåŠ¨ä¿å­˜çŠ¶æ€
5. æ”¯æŒéªŒè¯ç ç­‰æœåŠ¡å™¨å¼‚å¸¸çš„é‡è¯•å¤„ç†

æ–­ç‚¹æ¢å¤æœºåˆ¶ï¼š
1. progress.json ç²¾ç¡®è®°å½•æ¯ä¸ªplace_idçš„å¤„ç†çŠ¶æ€
2. é‡æ–°è¿è¡Œæ—¶è‡ªåŠ¨è·³è¿‡å·²æˆåŠŸå¤„ç†çš„åœ°ç‚¹
3. å¯é€‰æ‹©é‡è¯•å¤±è´¥çš„åœ°ç‚¹ï¼ˆ--retry-failedï¼‰
4. æ”¯æŒåˆ†æ‰¹å¤„ç†ï¼Œä¾¿äºç›‘æ§å¤§è§„æ¨¡æ•°æ®å¤„ç†

=============================================================================
"""

# =============================================================================
# é…ç½®åŒºåŸŸ - æ‰€æœ‰å¯è°ƒå‚æ•°
# =============================================================================

# é»˜è®¤å¤„ç†å‚æ•°é…ç½®
DEFAULT_CONFIG = {
    # === åŸºç¡€å¤„ç†å‚æ•° ===
    'max_workers': 3,           # é»˜è®¤æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå»ºè®®2-5ï¼Œè¿‡é«˜å¯èƒ½è¢«é™åˆ¶ï¼‰
    'max_retries': 3,           # é»˜è®¤æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå•ä¸ªåœ°ç‚¹çš„é‡è¯•æ¬¡æ•°ï¼‰
    'timeout': 300,             # é»˜è®¤å•ä¸ªåœ°ç‚¹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    'max_places': None,         # é»˜è®¤æ¯æ¬¡æœ€å¤§å¤„ç†åœ°ç‚¹æ•°é‡ï¼ˆNone=æ— é™åˆ¶ï¼Œå»ºè®®æµ‹è¯•æ—¶è®¾ç½®ä¸º5-10ï¼‰
    
    # === è¿›åº¦ç®¡ç†å‚æ•° ===
    'auto_save_interval': 10,   # è‡ªåŠ¨ä¿å­˜è¿›åº¦çš„é—´éš”ï¼ˆæ¯å¤„ç†Nä¸ªåœ°ç‚¹ä¿å­˜ä¸€æ¬¡ï¼‰
    'resume_by_default': True,  # é»˜è®¤æ˜¯å¦å¯ç”¨æ–­ç‚¹æ¢å¤
    'retry_failed_by_default': False,  # é»˜è®¤æ˜¯å¦é‡è¯•å¤±è´¥åœ°ç‚¹
    
    # === å¹¶å‘æ§åˆ¶å‚æ•° ===
    'queue_size_multiplier': 2, # å¹¶å‘é˜Ÿåˆ—å¤§å°å€æ•°ï¼ˆmax_workers * multiplierï¼‰
    'batch_submit_size': None,  # æ‰¹é‡æäº¤ä»»åŠ¡çš„å¤§å°ï¼ˆNone=ä½¿ç”¨é˜Ÿåˆ—å¤§å°ï¼‰
    
    # === æ–‡ä»¶å’Œæ—¥å¿—å‚æ•° ===
    'log_level': 'INFO',        # æ—¥å¿—çº§åˆ« (DEBUG, INFO, WARNING, ERROR)
    'filename_max_length': 50,  # æ–‡ä»¶åä¸­å•†æˆ·åç§°çš„æœ€å¤§é•¿åº¦
    'output_encoding': 'utf-8', # è¾“å‡ºæ–‡ä»¶ç¼–ç 
    
    # === çˆ¬è™«è„šæœ¬å‚æ•° ===
    'scraper_script': 'main.py', # å•ç‚¹çˆ¬è™«è„šæœ¬è·¯å¾„
    'headless_mode': True,      # æ˜¯å¦ä½¿ç”¨æ— ç•Œé¢æ¨¡å¼
    'verbose_logging': False,   # æ˜¯å¦å¯ç”¨è¯¦ç»†æ—¥å¿—
}

# æµ‹è¯•æ¨¡å¼é…ç½®ï¼ˆä¾¿äºå¿«é€Ÿæµ‹è¯•ï¼‰
TEST_CONFIG = {
    'max_workers': 1,           # æµ‹è¯•æ—¶ä½¿ç”¨å•çº¿ç¨‹
    'max_retries': 2,           # æµ‹è¯•æ—¶å‡å°‘é‡è¯•æ¬¡æ•°
    'timeout': 60,              # æµ‹è¯•æ—¶ç¼©çŸ­è¶…æ—¶æ—¶é—´
    'max_places': 5,            # æµ‹è¯•æ—¶åªå¤„ç†5ä¸ªåœ°ç‚¹
    'verbose_logging': True,    # æµ‹è¯•æ—¶å¯ç”¨è¯¦ç»†æ—¥å¿—
}

# ç”Ÿäº§ç¯å¢ƒé…ç½®ï¼ˆç”¨äºå¤§è§„æ¨¡æ•°æ®å¤„ç†ï¼‰
PRODUCTION_CONFIG = {
    'max_workers': 5,           # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨æ›´å¤šçº¿ç¨‹
    'max_retries': 5,           # ç”Ÿäº§ç¯å¢ƒå¢åŠ é‡è¯•æ¬¡æ•°
    'timeout': 600,             # ç”Ÿäº§ç¯å¢ƒå»¶é•¿è¶…æ—¶æ—¶é—´
    'max_places': 1000,         # ç”Ÿäº§ç¯å¢ƒæ¯æ¬¡å¤„ç†1000ä¸ªåœ°ç‚¹
    'auto_save_interval': 5,    # ç”Ÿäº§ç¯å¢ƒæ›´é¢‘ç¹ä¿å­˜è¿›åº¦
}

# æ— äººå€¼å®ˆæ¨¡å¼é…ç½®ï¼ˆé€‚åˆé•¿æœŸè‡ªåŠ¨è¿è¡Œï¼‰
UNATTENDED_CONFIG = {
    'max_workers': 6,           # ç¨³å®šçš„å¹¶å‘æ•°
    'max_places': 120,         # æ¯æ‰¹å¤„ç†2000ä¸ªåœ°ç‚¹
    'max_retries': 5,           # å¢åŠ é‡è¯•æ¬¡æ•°
    'timeout': 120,             # é•¿è¶…æ—¶æ—¶é—´
    'auto_save_interval': 12,   # æ›´é¢‘ç¹ä¿å­˜è¿›åº¦
    'auto_retry_failed': True,  # è‡ªåŠ¨é‡è¯•å¤±è´¥åœ°ç‚¹
    'continuous_mode': True,    # è¿ç»­å¤„ç†æ¨¡å¼
    'max_continuous_rounds': 5, # æœ€å¤§è¿ç»­å¤„ç†è½®æ•°ï¼ˆ-1è¡¨ç¤ºæ— é™åˆ¶ï¼‰
    'rest_between_rounds': 60, # è½®æ¬¡é—´ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
    'infinite_mode': False,     # æ— é™æ¨¡å¼ï¼ˆå¿½ç•¥max_continuous_roundsï¼‰
    'max_idle_rounds': 3,       # æœ€å¤§ç©ºé—²è½®æ•°ï¼ˆæ²¡æœ‰æ–°åœ°ç‚¹å¤„ç†æ—¶åœæ­¢ï¼‰
    'checkpoint_interval': 60,  # æ£€æŸ¥ç‚¹é—´éš”æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
    'enable_keyboard_interrupt': True,  # å…è®¸Ctrl+Cä¼˜é›…é€€å‡º
}

# ğŸ†• æ— é™è¿è¡Œæ¨¡å¼é…ç½®
INFINITE_CONFIG = {
    'max_workers': 6,           # é™ä½å¹¶å‘ä»¥ç¨³å®šè¿è¡Œ
    'max_places': 300,         # æ¯æ‰¹å¤„ç†300ä¸ªåœ°ç‚¹
    'max_retries': 3,           # é€‚ä¸­çš„é‡è¯•æ¬¡æ•°
    'timeout': 150,             # é€‚ä¸­çš„è¶…æ—¶æ—¶é—´
    'auto_save_interval': 6,    # æ›´é¢‘ç¹ä¿å­˜è¿›åº¦
    'auto_retry_failed': True,  # è‡ªåŠ¨é‡è¯•å¤±è´¥åœ°ç‚¹
    'continuous_mode': True,    # è¿ç»­å¤„ç†æ¨¡å¼
    'max_continuous_rounds': -1, # æ— é™è½®æ•°
    'rest_between_rounds': 60, # è½®æ¬¡é—´ä¼‘æ¯3åˆ†é’Ÿ
    'infinite_mode': True,      # ğŸ”¥ å¼€å¯æ— é™æ¨¡å¼
    'max_idle_rounds': 5,       # è¿ç»­5è½®æ— æ–°åœ°ç‚¹æ—¶åœæ­¢
    'checkpoint_interval': 30,  # æ¯30åˆ†é’Ÿè¾“å‡ºæ£€æŸ¥ç‚¹ä¿¡æ¯
    'enable_keyboard_interrupt': True,  # å…è®¸Ctrl+Cä¼˜é›…é€€å‡º
}

# ğŸ†• å¿«é€Ÿæ¨¡å¼é…ç½®ï¼ˆé€‚åˆæµ‹è¯•æˆ–å°è§„æ¨¡æ•°æ®ï¼‰
FAST_CONFIG = {
    'max_workers': 5,           # æ›´é«˜çš„å¹¶å‘æ•°
    'max_places': 500,          # æ¯æ‰¹å¤„ç†500ä¸ªåœ°ç‚¹
    'max_retries': 2,           # è¾ƒå°‘çš„é‡è¯•æ¬¡æ•°
    'timeout': 200,             # è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
    'auto_save_interval': 20,   # é€‚ä¸­çš„ä¿å­˜é—´éš”
    'auto_retry_failed': False, # ä¸è‡ªåŠ¨é‡è¯•ï¼Œæé«˜é€Ÿåº¦
    'continuous_mode': True,    # è¿ç»­å¤„ç†æ¨¡å¼
    'max_continuous_rounds': 10, # æœ€å¤š10è½®
    'rest_between_rounds': 60,  # è½®æ¬¡é—´ä¼‘æ¯1åˆ†é’Ÿ
    'infinite_mode': False,     # éæ— é™æ¨¡å¼
}

# =============================================================================
# å¯¼å…¥ä¾èµ–æ¨¡å—
# =============================================================================

import json
import os
import time
import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Set
from tqdm import tqdm
import subprocess
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# =============================================================================
# ä¸»è¦ç±»å®šä¹‰
# =============================================================================

class BatchGoogleMapsScraper:
    """
    å¤§è§„æ¨¡æ‰¹é‡Google Mapsæ•°æ®çˆ¬å–å™¨
    
    ä¸“ä¸ºå¤„ç†æ•°ä¸‡çº§åœ°ç‚¹æ•°æ®è®¾è®¡ï¼Œå…·å¤‡å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶
    """
    
    def __init__(self, script_path: Optional[str] = None, output_base_dir: str = "batch_output", config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æ‰¹é‡çˆ¬è™«
        
        Args:
            script_path: å•ç‚¹çˆ¬è™«è„šæœ¬è·¯å¾„
            output_base_dir: è¾“å‡ºæ ¹ç›®å½•
            config: é…ç½®å‚æ•°å­—å…¸ï¼Œè¦†ç›–é»˜è®¤é…ç½®
        """
        # åˆå¹¶é…ç½®å‚æ•°
        self.config = DEFAULT_CONFIG.copy()
        if config:
            self.config.update(config)
        
        # æ ¸å¿ƒè·¯å¾„è®¾ç½®
        self.script_path = script_path or self.config['scraper_script']
        self.output_base_dir = Path(output_base_dir)
        
        # çŠ¶æ€æ–‡ä»¶è·¯å¾„
        self.progress_file = self.output_base_dir / "progress.json"      # è¿›åº¦æ–‡ä»¶
        self.error_log_file = self.output_base_dir / "errors.jsonl"     # é”™è¯¯æ—¥å¿—
        self.success_log_file = self.output_base_dir / "success.jsonl"  # æˆåŠŸæ—¥å¿—
        
        # çº¿ç¨‹å®‰å…¨é”ï¼ˆç”¨äºæ–‡ä»¶å†™å…¥ä¿æŠ¤ï¼‰
        self.lock = threading.Lock()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_base_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        self.setup_logging()
        
        # è¾“å‡ºåˆå§‹åŒ–ä¿¡æ¯
        self.logger.info(f"æ‰¹é‡çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_base_dir}")
        self.logger.info(f"å½“å‰é…ç½®: å¹¶å‘={self.config['max_workers']}, é‡è¯•={self.config['max_retries']}, è¶…æ—¶={self.config['timeout']}s")
        if self.config['max_places']:
            self.logger.info(f"å¤„ç†æ•°é‡é™åˆ¶: {self.config['max_places']} ä¸ªåœ°ç‚¹")
        
    def setup_logging(self):
        """
        è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        
        åˆ›å»ºæ–‡ä»¶å’Œæ§åˆ¶å°åŒé‡æ—¥å¿—è¾“å‡ºï¼Œä¾¿äºè°ƒè¯•å’Œç›‘æ§
        """
        log_file = self.output_base_dir / "batch_scraper.log"
        
        # æ ¹æ®é…ç½®è®¾ç½®æ—¥å¿—çº§åˆ«
        log_level = getattr(logging, self.config['log_level'].upper())
        
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),  # æ–‡ä»¶æ—¥å¿—
                logging.StreamHandler(sys.stdout)                # æ§åˆ¶å°æ—¥å¿—
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_jsonl_input(self, input_file: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½JSONLæ ¼å¼çš„è¾“å…¥æ–‡ä»¶
        
        æ”¯æŒåŒ…å«place_id, Maps_url, parent_type, sub_typeç­‰å­—æ®µçš„åœ°ç‚¹æ•°æ®
        
        Args:
            input_file: JSONLæ–‡ä»¶è·¯å¾„
            
        Returns:
            åœ°ç‚¹æ•°æ®åˆ—è¡¨
        """
        places = []
        try:
            # ğŸ”¥ æ”¹è¿›ç¼–ç å¤„ç†ï¼Œå°è¯•å¤šç§ç¼–ç æ–¹å¼
            encodings_to_try = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312']
            file_content = None
            used_encoding = None
            
            for encoding in encodings_to_try:
                try:
                    with open(input_file, 'r', encoding=encoding) as f:
                        file_content = f.read()
                        used_encoding = encoding
                        break
                except (UnicodeDecodeError, UnicodeError):
                    continue
            
            if file_content is None:
                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶å¿½ç•¥é”™è¯¯
                with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:
                    file_content = f.read()
                    used_encoding = 'utf-8 (with errors ignored)'
            
            self.logger.info(f"ä½¿ç”¨ç¼–ç  {used_encoding} è¯»å–è¾“å…¥æ–‡ä»¶")
            
            # é€è¡Œè§£æJSONL
            for line_num, line in enumerate(file_content.splitlines(), 1):
                line = line.strip()
                if not line:
                    continue
                try:
                    place_data = json.loads(line)
                    # éªŒè¯å¿…éœ€å­—æ®µ
                    required_fields = ['place_id', 'Maps_url', 'parent_type', 'sub_type']
                    if all(field in place_data for field in required_fields):
                        places.append(place_data)
                    else:
                        missing_fields = [f for f in required_fields if f not in place_data]
                        self.logger.warning(f"ç¬¬{line_num}è¡Œç¼ºå°‘å¿…éœ€å­—æ®µ {missing_fields}: {line[:100]}...")
                except json.JSONDecodeError as e:
                    self.logger.error(f"ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
        except FileNotFoundError:
            self.logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            return []
        except Exception as e:
            self.logger.error(f"è¯»å–è¾“å…¥æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []
        
        self.logger.info(f"æˆåŠŸåŠ è½½ {len(places)} ä¸ªåœ°ç‚¹æ•°æ®")
        return places
    
    def load_progress(self) -> Dict[str, Any]:
        """
        åŠ è½½å¤„ç†è¿›åº¦
        
        ä»progress.jsonè¯»å–ä¹‹å‰çš„å¤„ç†çŠ¶æ€ï¼Œæ”¯æŒæ–­ç‚¹æ¢å¤
        
        Returns:
            åŒ…å«å¤„ç†è¿›åº¦çš„å­—å…¸
        """
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                    
                    # ç¡®ä¿åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼ˆå‘åå…¼å®¹ï¼‰
                    default_progress = {
                        "total": 0,                 # æ€»åœ°ç‚¹æ•°
                        "current_index": 0,         # å½“å‰å¤„ç†ç´¢å¼•
                        "successful": [],           # æˆåŠŸå¤„ç†çš„place_idåˆ—è¡¨
                        "failed": [],               # å¤±è´¥çš„place_idåˆ—è¡¨
                        "skipped": [],              # è·³è¿‡çš„place_idåˆ—è¡¨
                        "success_count": 0,         # æˆåŠŸè®¡æ•°
                        "failed_count": 0,          # å¤±è´¥è®¡æ•°
                        "skipped_count": 0,         # è·³è¿‡è®¡æ•°
                        "last_updated": "",         # æœ€åæ›´æ–°æ—¶é—´
                        "start_time": "",           # å¼€å§‹æ—¶é—´
                        "session_info": {}          # ä¼šè¯ä¿¡æ¯
                    }
                    
                    # åˆå¹¶åŠ è½½çš„æ•°æ®å’Œé»˜è®¤æ•°æ®ï¼ˆç¡®ä¿å­—æ®µå®Œæ•´ï¼‰
                    for key, value in default_progress.items():
                        if key not in progress:
                            progress[key] = value
                    
                    self.logger.info(f"åŠ è½½è¿›åº¦: æˆåŠŸ {len(progress['successful'])}, å¤±è´¥ {len(progress['failed'])}")
                    return progress
                    
            except Exception as e:
                self.logger.warning(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹")
        
        # è¿”å›ç©ºè¿›åº¦ï¼ˆé¦–æ¬¡è¿è¡Œæˆ–è¿›åº¦æ–‡ä»¶æŸåï¼‰
        return {
            "total": 0,
            "current_index": 0,
            "successful": [],
            "failed": [],
            "skipped": [],
            "success_count": 0,
            "failed_count": 0,
            "skipped_count": 0,
            "last_updated": "",
            "start_time": "",
            "session_info": {}
        }
    
    def save_progress(self, progress: Dict[str, Any]):
        """
        ä¿å­˜å¤„ç†è¿›åº¦
        
        çº¿ç¨‹å®‰å…¨åœ°ä¿å­˜å½“å‰å¤„ç†çŠ¶æ€åˆ°progress.json
        
        Args:
            progress: è¿›åº¦ä¿¡æ¯å­—å…¸
        """
        progress["last_updated"] = time.strftime("%Y-%m-%d %H:%M:%S")
        
        with self.lock:  # ç¡®ä¿æ–‡ä»¶å†™å…¥çš„çº¿ç¨‹å®‰å…¨
            try:
                with open(self.progress_file, 'w', encoding='utf-8') as f:
                    json.dump(progress, f, ensure_ascii=False, indent=2)
                self.logger.debug("è¿›åº¦å·²ä¿å­˜")
            except Exception as e:
                self.logger.error(f"ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def get_processed_place_ids(self) -> Set[str]:
        """
        è·å–æ‰€æœ‰å·²å¤„ç†ï¼ˆæˆåŠŸ+å¤±è´¥+è·³è¿‡ï¼‰çš„place_idé›†åˆ
        
        ç”¨äºåˆ¤æ–­å“ªäº›åœ°ç‚¹å·²ç»å¤„ç†è¿‡ï¼Œé¿å…é‡å¤å¤„ç†
        
        Returns:
            å·²å¤„ç†çš„place_idé›†åˆ
        """
        progress = self.load_progress()
        processed = set(progress.get("successful", []))
        processed.update(progress.get("failed", []))
        processed.update(progress.get("skipped", []))
        return processed
    
    def get_failed_place_ids(self) -> Set[str]:
        """
        è·å–å¤±è´¥çš„place_idé›†åˆ
        
        ç”¨äºé‡è¯•æ¨¡å¼ï¼Œåªé‡è¯•å¤±è´¥çš„åœ°ç‚¹
        
        Returns:
            å¤±è´¥çš„place_idé›†åˆ
        """
        progress = self.load_progress()
        return set(progress.get("failed", []))
    
    def filter_places_to_process(self, places: List[Dict[str, Any]], 
                                retry_failed: bool = False, 
                                max_places: Optional[int] = None) -> tuple[List[Dict[str, Any]], Set[str]]:
        """
        è¿‡æ»¤éœ€è¦å¤„ç†çš„åœ°ç‚¹
        
        æ ¹æ®å·²å¤„ç†çŠ¶æ€å’Œé…ç½®å‚æ•°ï¼Œç¡®å®šå®é™…éœ€è¦å¤„ç†çš„åœ°ç‚¹åˆ—è¡¨
        
        Args:
            places: æ‰€æœ‰åœ°ç‚¹åˆ—è¡¨
            retry_failed: æ˜¯å¦é‡è¯•å¤±è´¥çš„åœ°ç‚¹
            max_places: æœ€å¤§å¤„ç†æ•°é‡é™åˆ¶
            
        Returns:
            tuple: (éœ€è¦å¤„ç†çš„åœ°ç‚¹åˆ—è¡¨, å¤±è´¥çš„place_idé›†åˆ)
        """
        # ğŸ”¥ ä¼˜åŒ–ï¼šåªåŠ è½½ä¸€æ¬¡è¿›åº¦ä¿¡æ¯ï¼Œé¿å…é‡å¤æ—¥å¿—
        progress = self.load_progress()
        processed_ids = set(progress.get("successful", []))
        processed_ids.update(progress.get("failed", []))
        processed_ids.update(progress.get("skipped", []))
        failed_ids = set(progress.get("failed", []))
        
        to_process = []
        skipped_already_processed = 0
        skipped_not_retry = 0
        
        for place in places:
            place_id = place["place_id"]
            
            if place_id not in processed_ids:
                # æœªå¤„ç†è¿‡çš„åœ°ç‚¹ - æ·»åŠ åˆ°å¤„ç†åˆ—è¡¨
                to_process.append(place)
            elif retry_failed and place_id in failed_ids:
                # é‡è¯•æ¨¡å¼ä¸‹çš„å¤±è´¥åœ°ç‚¹ - æ·»åŠ åˆ°å¤„ç†åˆ—è¡¨
                to_process.append(place)
            elif place_id in failed_ids:
                # éé‡è¯•æ¨¡å¼ä¸‹çš„å¤±è´¥åœ°ç‚¹ - è·³è¿‡
                skipped_not_retry += 1
            else:
                # å·²æˆåŠŸå¤„ç†çš„åœ°ç‚¹ - è·³è¿‡
                skipped_already_processed += 1
        
        original_count = len(to_process)
        
        # ğŸ”¥ æ–°å¢ï¼šè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        total_input = len(places)
        self.logger.info(f"åœ°ç‚¹è¿‡æ»¤ç»Ÿè®¡:")
        self.logger.info(f"  è¾“å…¥æ€»æ•°: {total_input}")
        self.logger.info(f"  å·²æˆåŠŸå¤„ç†: {len(progress.get('successful', []))}")
        self.logger.info(f"  å·²å¤±è´¥: {len(failed_ids)}")
        self.logger.info(f"  éœ€è¦å¤„ç†: {original_count}")
        if skipped_already_processed > 0:
            self.logger.info(f"  è·³è¿‡(å·²å¤„ç†): {skipped_already_processed}")
        if skipped_not_retry > 0:
            self.logger.info(f"  è·³è¿‡(å¤±è´¥ä½†éé‡è¯•): {skipped_not_retry}")
        
        # åº”ç”¨æ•°é‡é™åˆ¶
        max_places = max_places or self.config['max_places']
        if max_places and max_places > 0:
            to_process = to_process[:max_places]
            if len(to_process) < original_count:
                self.logger.info(f"åº”ç”¨æ•°é‡é™åˆ¶: {original_count} -> {len(to_process)} ä¸ªåœ°ç‚¹")
        
        self.logger.info(f"æœ€ç»ˆå¾…å¤„ç†åœ°ç‚¹: {len(to_process)} ä¸ª")
        if retry_failed:
            retry_count = sum(1 for place in to_process if place["place_id"] in failed_ids)
            self.logger.info(f"å…¶ä¸­é‡è¯•å¤±è´¥åœ°ç‚¹: {retry_count} ä¸ª")
        
        return to_process, failed_ids
    
    def get_output_path(self, parent_type: str, sub_type: str) -> Path:
        """
        è·å–è¾“å‡ºç›®å½•è·¯å¾„
        
        æ ¹æ®åœ°ç‚¹ç±»å‹åˆ›å»ºå¯¹åº”çš„ç›®å½•ç»“æ„
        
        Args:
            parent_type: çˆ¶ç±»åˆ«ï¼ˆå¦‚ï¼šentertainment_and_recreationï¼‰
            sub_type: å­ç±»åˆ«ï¼ˆå¦‚ï¼šcycling_parkï¼‰
            
        Returns:
            è¾“å‡ºç›®å½•è·¯å¾„
        """
        type_dir = self.output_base_dir / parent_type / sub_type
        type_dir.mkdir(parents=True, exist_ok=True)
        return type_dir
    
    def generate_filename(self, place_id: str, business_name: str = "") -> str:
        """
        ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        
        æ¸…ç†å•†æˆ·åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œç¡®ä¿æ–‡ä»¶ååœ¨å„æ“ä½œç³»ç»Ÿä¸‹éƒ½å¯ç”¨
        
        Args:
            place_id: åœ°ç‚¹ID
            business_name: å•†æˆ·åç§°
            
        Returns:
            å®‰å…¨çš„æ–‡ä»¶å
        """
        import re
        
        # æ¸…ç†å•†æˆ·åç§°ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        if business_name:
            # æ›¿æ¢æ‰€æœ‰éå­—æ¯æ•°å­—çš„å­—ç¬¦ä¸ºä¸‹åˆ’çº¿ï¼ŒåŒ…æ‹¬ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·
            safe_name = re.sub(r'[<>:"/\\|?*\s&/().,;!\[\]{}]', '_', business_name)
            # ç§»é™¤å¤šä¸ªè¿ç»­çš„ä¸‹åˆ’çº¿
            safe_name = re.sub(r'_+', '_', safe_name)
            # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿
            safe_name = safe_name.strip('_')
            # é™åˆ¶é•¿åº¦é¿å…æ–‡ä»¶åè¿‡é•¿
            safe_name = safe_name[:self.config['filename_max_length']] if len(safe_name) > self.config['filename_max_length'] else safe_name
            filename = f"{place_id}_{safe_name}.json"
        else:
            filename = f"{place_id}.json"
        
        return filename
    
    def scrape_single_place(self, place_data: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """
        çˆ¬å–å•ä¸ªåœ°ç‚¹æ•°æ®
        
        è°ƒç”¨å•ç‚¹çˆ¬è™«è„šæœ¬ï¼Œå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µï¼Œç¡®ä¿å•ç‚¹å¤±è´¥ä¸å½±å“æ•´ä½“å¤„ç†
        
        Args:
            place_data: åœ°ç‚¹ä¿¡æ¯å­—å…¸
            **kwargs: ä¼ é€’ç»™çˆ¬è™«çš„é¢å¤–å‚æ•°
            
        Returns:
            åŒ…å«çˆ¬å–ç»“æœçš„å­—å…¸
            
        å¼‚å¸¸å¤„ç†æœºåˆ¶ï¼š
        1. è„šæœ¬æ‰§è¡Œå¤±è´¥ - è®°å½•é”™è¯¯ä¿¡æ¯ï¼Œè¿”å›å¤±è´¥çŠ¶æ€
        2. JSONè§£æå¤±è´¥ - è®°å½•è§£æé”™è¯¯ï¼Œè¿”å›å¤±è´¥çŠ¶æ€
        3. è¶…æ—¶å¼‚å¸¸ - è®°å½•è¶…æ—¶ä¿¡æ¯ï¼Œå¯é‡è¯•
        4. å…¶ä»–å¼‚å¸¸ - ç»Ÿä¸€æ•è·å¹¶è®°å½•
        """
        url = place_data["Maps_url"]
        place_id = place_data["place_id"]
        
        # æ„å»ºçˆ¬è™«å‘½ä»¤
        cmd = [
            sys.executable, self.script_path,
            "--url", url,
            "--output-dir", str(self.output_base_dir),  # æ·»åŠ è¾“å‡ºç›®å½•å‚æ•°
            "--json-output",
        ]
        
        # æ ¹æ®é…ç½®æ·»åŠ æ— ç•Œé¢æ¨¡å¼
        if self.config['headless_mode']:
            cmd.append("--headless")
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        if kwargs.get("max_retries"):
            cmd.extend(["--max-retries", str(kwargs["max_retries"])])
        if kwargs.get("verbose") or self.config['verbose_logging']:
            cmd.append("--verbose")
        
        try:
            # æ‰§è¡Œçˆ¬è™«è„šæœ¬
            # ä½¿ç”¨subprocessç¡®ä¿è¿›ç¨‹éš”ç¦»ï¼Œå•ä¸ªåœ°ç‚¹å¤±è´¥ä¸å½±å“å…¶ä»–åœ°ç‚¹
            
            # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ç¡®ä¿æ­£ç¡®çš„ç¼–ç 
            env = os.environ.copy()
            env['PYTHONIOENCODING'] = 'utf-8'
            env['PYTHONLEGACYWINDOWSSTDIO'] = '0'  # Windowsä¸‹å¼ºåˆ¶ä½¿ç”¨UTF-8
            if sys.platform.startswith('win'):
                env['CHCP'] = '65001'  # Windowsä»£ç é¡µè®¾ç½®ä¸ºUTF-8
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                encoding='utf-8',  # æ˜ç¡®ä½¿ç”¨UTF-8ç¼–ç 
                errors='ignore',   # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šå¿½ç•¥æ— æ³•è§£ç çš„å­—ç¬¦ï¼Œè€Œä¸æ˜¯æ›¿æ¢
                timeout=kwargs.get("timeout", self.config['timeout']),
                env=env  # ğŸ”¥ ä¼ é€’ç¯å¢ƒå˜é‡
            )
            
            if result.returncode == 0:
                # è„šæœ¬æ‰§è¡ŒæˆåŠŸï¼Œè§£æç»“æœ
                try:
                    # ğŸ†• æ”¹è¿›JSONè§£æï¼Œå¤„ç†ç¼–ç é—®é¢˜
                    output_text = result.stdout.strip()
                    
                    # å¦‚æœè¾“å‡ºä¸ºç©ºï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                    if not output_text:
                        self.logger.warning(f"è„šæœ¬è¾“å‡ºä¸ºç©º {place_id}")
                        scrape_result = {"success": False, "error_code": 1999, "error_message": "è„šæœ¬è¾“å‡ºä¸ºç©º"}
                    else:
                        # ğŸ”¥ æ”¹è¿›çš„JSONè§£æé€»è¾‘ - å¯»æ‰¾JSONå¼€å§‹ä½ç½®
                        try:
                            # å¯»æ‰¾JSONæ•°ç»„å¼€å§‹ä½ç½®
                            json_start = output_text.find('[')
                            if json_start != -1:
                                # æå–ä»JSONå¼€å§‹çš„éƒ¨åˆ†
                                json_part = output_text[json_start:].strip()
                                output_data = json.loads(json_part)
                                scrape_result = output_data[0] if output_data else {}
                                
                                self.logger.debug(f"JSONè§£ææˆåŠŸ {place_id}: {scrape_result.get('business_info', {}).get('name', 'æœªçŸ¥')}")
                                
                            elif output_text.startswith('{'):
                                # å¤„ç†å•ä¸ªJSONå¯¹è±¡çš„æƒ…å†µ
                                output_data = json.loads(output_text)
                                scrape_result = output_data
                            else:
                                # æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ‡è®°
                                self.logger.error(f"æœªæ‰¾åˆ°æœ‰æ•ˆJSON {place_id}")
                                self.logger.error(f"åŸå§‹è¾“å‡º: {output_text[:200]}...")
                                scrape_result = {"success": False, "error_code": 1999, "error_message": "è¾“å‡ºä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSON"}
                                
                        except json.JSONDecodeError as e:
                            # JSONè§£æå¤±è´¥æ—¶ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
                            self.logger.error(f"JSONè§£æå¤±è´¥ {place_id}: {e}")
                            
                            # å¦‚æœæ‰¾åˆ°äº†JSONå¼€å§‹ä½ç½®ï¼Œè®°å½•å°è¯•è§£æçš„å†…å®¹
                            json_start = output_text.find('[')
                            if json_start != -1:
                                json_part = output_text[json_start:]
                                self.logger.error(f"å°è¯•è§£æçš„JSONå‰200å­—ç¬¦: {json_part[:200]}")
                            else:
                                self.logger.error(f"åŸå§‹è¾“å‡ºå‰200å­—ç¬¦: {output_text[:200]}")
                            
                            # å°è¯•æå–å¯èƒ½çš„é”™è¯¯ä¿¡æ¯
                            error_msg = f"JSONè§£æå¤±è´¥: {e}"
                            if "encoding" in str(e).lower() or "codec" in str(e).lower():
                                error_msg += " (å¯èƒ½æ˜¯ç¼–ç é—®é¢˜)"
                            
                            scrape_result = {"success": False, "error_code": 1999, "error_message": error_msg}
                
                except Exception as e:
                    # å…¶ä»–è§£æå¼‚å¸¸
                    self.logger.error(f"è¾“å‡ºè§£æå¼‚å¸¸ {place_id}: {e}")
                    scrape_result = {"success": False, "error_code": 1999, "error_message": f"è¾“å‡ºè§£æå¼‚å¸¸: {e}"}
                
                # åˆå¹¶åŸå§‹æ•°æ®å’Œçˆ¬å–ç»“æœ
                combined_result = {
                    **place_data,  # ä¿ç•™åŸå§‹çš„place_id, parent_typeç­‰ä¿¡æ¯
                    "scrape_success": scrape_result.get("success", False),
                    "scrape_error_code": scrape_result.get("error_code", 0),
                    "scrape_error_message": scrape_result.get("error_message", ""),
                    "business_info": scrape_result.get("business_info", {}),
                    "reviews_count": scrape_result.get("reviews_count", 0),
                    "reviews": scrape_result.get("reviews", []),
                    "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "retry_attempt": kwargs.get("retry_attempt", 0)
                }
                
                # ğŸ†• å¢å¼ºè¯„è®ºçŠ¶æ€åˆ¤æ–­
                if combined_result["scrape_success"]:
                    reviews_count = combined_result["reviews_count"]
                    error_code = combined_result["scrape_error_code"]
                    
                    if reviews_count > 0:
                        # æœ‰è¯„è®ºçš„æƒ…å†µ - çœŸæ­£æˆåŠŸ
                        combined_result["has_reviews"] = True
                        combined_result["review_status"] = "æœ‰è¯„è®º"
                        combined_result["final_success"] = True
                    elif error_code == 1004:
                        # æ‰¾ä¸åˆ°ReviewsæŒ‰é’®ï¼Œè¯´æ˜å•†æˆ·æ²¡æœ‰è¯„è®ºåŠŸèƒ½ï¼ˆæ­£å¸¸æƒ…å†µï¼‰- ç®—æˆåŠŸ
                        combined_result["has_reviews"] = False
                        combined_result["review_status"] = "æ— è¯„è®ºåŠŸèƒ½"
                        combined_result["final_success"] = True
                    else:
                        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šæ”¹è¿›æ— è¯„è®ºåœ°ç‚¹çš„åˆ¤æ–­é€»è¾‘
                        if error_code == 0 and reviews_count == 0:
                            # æˆåŠŸæŠ“å–åœ°ç‚¹ä¿¡æ¯ä½†æ²¡æœ‰è¯„è®ºï¼Œè¿™å¯èƒ½æ˜¯æ­£å¸¸æƒ…å†µï¼ˆåœ°ç‚¹ç¡®å®æ²¡æœ‰è¯„è®ºï¼‰
                            # ä¸åº”è¯¥æ— é™é‡è¯•ï¼Œåº”è¯¥æ ‡è®°ä¸ºæˆåŠŸ
                            combined_result["has_reviews"] = False
                            combined_result["review_status"] = "æš‚æ— è¯„è®º"
                            combined_result["final_success"] = True  # ğŸ”¥ æ”¹ä¸ºæˆåŠŸï¼Œé¿å…æ— é™é‡è¯•
                        else:
                            combined_result["has_reviews"] = False
                            combined_result["review_status"] = "è¯„è®ºæŠ“å–å¤±è´¥"
                            combined_result["final_success"] = False  # çœŸæ­£çš„å¤±è´¥æ‰éœ€è¦é‡è¯•
                else:
                    # æŠ“å–å¤±è´¥çš„æƒ…å†µ
                    combined_result["has_reviews"] = None
                    combined_result["review_status"] = "æŠ“å–å¤±è´¥"
                    combined_result["final_success"] = False
                
                return combined_result
            else:
                # è„šæœ¬æ‰§è¡Œå¤±è´¥ - å¯èƒ½æ˜¯æµè§ˆå™¨å¯åŠ¨å¤±è´¥ã€é¡µé¢åŠ è½½å¤±è´¥ç­‰
                error_msg = result.stderr[:500] if result.stderr else "æœªçŸ¥é”™è¯¯"
                failed_result = {
                    **place_data,
                    "scrape_success": False,
                    "scrape_error_code": 1999,
                    "scrape_error_message": f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {error_msg}",
                    "business_info": {},
                    "reviews_count": 0,
                    "reviews": [],
                    "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "retry_attempt": kwargs.get("retry_attempt", 0),
                    "has_reviews": None,
                    "review_status": "æŠ“å–å¤±è´¥",
                    "final_success": False
                }
                return failed_result
                
        except subprocess.TimeoutExpired:
            # è¶…æ—¶å¼‚å¸¸ - ç½‘ç»œæ…¢æˆ–é¡µé¢åŠ è½½æ—¶é—´è¿‡é•¿ï¼Œè¿™ç§æƒ…å†µå¯ä»¥é‡è¯•
            timeout_result = {
                **place_data,
                "scrape_success": False,
                "scrape_error_code": 1009,
                "scrape_error_message": "çˆ¬å–è¶…æ—¶",
                "business_info": {},
                "reviews_count": 0,
                "reviews": [],
                "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "retry_attempt": kwargs.get("retry_attempt", 0),
                "has_reviews": None,
                "review_status": "è¶…æ—¶å¤±è´¥",
                "final_success": False
            }
            return timeout_result
        except Exception as e:
            # å…¶ä»–æœªé¢„æœŸçš„å¼‚å¸¸
            exception_result = {
                **place_data,
                "scrape_success": False,
                "scrape_error_code": 1999,
                "scrape_error_message": f"æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                "business_info": {},
                "reviews_count": 0,
                "reviews": [],
                "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                "retry_attempt": kwargs.get("retry_attempt", 0),
                "has_reviews": None,
                "review_status": "æ‰§è¡Œå¼‚å¸¸",
                "final_success": False
            }
            return exception_result
    
    def _cleanup_csv_file(self, result: Dict[str, Any], business_name: str):
        """
        æ¸…ç†main.pyç”Ÿæˆçš„CSVæ–‡ä»¶
        
        åœ¨JSONæ–‡ä»¶æˆåŠŸä¿å­˜åï¼Œåˆ é™¤å¯¹åº”çš„CSVæ–‡ä»¶ä»¥ä¿æŒç›®å½•æ•´æ´
        å°è¯•å¤šç§æ–¹å¼ç¡®å®šå¯èƒ½çš„CSVæ–‡ä»¶å
        
        Args:
            result: å¤„ç†ç»“æœå­—å…¸
            business_name: ä»business_infoè·å–çš„å•†æˆ·åç§°
        """
        import re
        
        # æ”¶é›†å¯èƒ½çš„å•†æˆ·åç§°
        possible_names = []
        place_id = result.get("place_id", "")
        maps_url = result.get("Maps_url", "")
        
        self.logger.debug(f"å¼€å§‹æ¸…ç†CSVæ–‡ä»¶ï¼Œplace_id: {place_id}")
        
        # 1. ä»business_infoè·å–çš„åç§°
        if business_name:
            possible_names.append(business_name)
            self.logger.debug(f"æ·»åŠ å•†æˆ·åç§°: {business_name}")
        
        # 2. æ£€æŸ¥ç°æœ‰CSVæ–‡ä»¶ï¼Œçœ‹æ˜¯å¦æœ‰åŒ¹é…çš„place_id
        if place_id and maps_url:
            # ä»URLä¸­æå–cid
            cid_match = re.search(r'cid=(\d+)', maps_url)
            if cid_match:
                cid = cid_match.group(1)
                self.logger.debug(f"æå–åˆ°CID: {cid}")
                
                # éå†è¾“å‡ºç›®å½•ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶
                for csv_file in self.output_base_dir.glob("*.csv"):
                    try:
                        # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦åŒ…å«è¿™ä¸ªcid
                        # ğŸ”¥ æ”¹è¿›ç¼–ç å¤„ç†ï¼Œå°è¯•å¤šç§ç¼–ç æ–¹å¼
                        encodings_to_try = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'latin-1']
                        content = None
                        
                        for encoding in encodings_to_try:
                            try:
                                with open(csv_file, 'r', encoding=encoding, errors='ignore') as f:
                                    content = f.read(500)  # è¯»å–å‰500å­—ç¬¦
                                    break
                            except (UnicodeDecodeError, UnicodeError):
                                continue
                        
                        if content and cid in content:
                            csv_name = csv_file.stem  # æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
                            possible_names.append(csv_name)
                            self.logger.debug(f"é€šè¿‡CIDåŒ¹é…åˆ°CSVæ–‡ä»¶: {csv_name}")
                            break
                    except Exception as e:
                        self.logger.debug(f"è¯»å–CSVæ–‡ä»¶ {csv_file.name} å¤±è´¥: {e}")
                        continue
        
        # 3. å°è¯•æ¸…ç†æ‰€æœ‰å¯èƒ½çš„CSVæ–‡ä»¶
        cleaned = False
        for name in possible_names:
            if name:
                try:
                    # å¤åˆ¶main.pyä¸­çš„æ–‡ä»¶åç”Ÿæˆé€»è¾‘
                    safe_name = re.sub(r'[<>:"/\\|?*]', '_', name)
                    csv_filename = f"{safe_name}.csv"
                    csv_file_path = self.output_base_dir / csv_filename
                    
                    self.logger.debug(f"å°è¯•æ¸…ç†CSVæ–‡ä»¶: {csv_filename}")
                    
                    # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶åˆ é™¤
                    if csv_file_path.exists():
                        try:
                            csv_file_path.unlink()  # åˆ é™¤æ–‡ä»¶
                            self.logger.debug(f"å·²æ¸…ç†CSVæ–‡ä»¶: {csv_filename}")
                            cleaned = True
                            return  # æˆåŠŸæ¸…ç†ä¸€ä¸ªå°±å¤Ÿäº†
                        except Exception as e:
                            self.logger.warning(f"æ¸…ç†CSVæ–‡ä»¶å¤±è´¥ {csv_filename}: {e}")
                    else:
                        self.logger.debug(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_filename}")
                    
                except Exception as e:
                    self.logger.debug(f"CSVæ–‡ä»¶æ¸…ç†è¿‡ç¨‹å‡ºé”™ {name}: {e}")
                    continue
        
        if not cleaned:
            self.logger.debug(f"æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„CSVæ–‡ä»¶ï¼Œplace_id: {place_id}")
    
    def save_result(self, result: Dict[str, Any]):
        """
        ä¿å­˜å•ä¸ªç»“æœåˆ°ç‹¬ç«‹çš„JSONæ–‡ä»¶
        
        ä¸ºæ¯ä¸ªåœ°ç‚¹åˆ›å»ºç‹¬ç«‹çš„JSONæ–‡ä»¶ï¼Œé¿å…å¤§æ–‡ä»¶é—®é¢˜ï¼Œæ”¯æŒé«˜å¹¶å‘
        åŒæ—¶æ¸…ç†main.pyç”Ÿæˆçš„CSVæ–‡ä»¶ï¼Œä¿æŒè¾“å‡ºç›®å½•æ•´æ´
        ğŸ”¥ å¢å¼ºç‰ˆï¼šç¡®ä¿ç«‹å³åˆ·æ–°åˆ°ç£ç›˜ï¼Œé˜²æ­¢æ„å¤–ä¸­æ–­ä¸¢å¤±æ•°æ®
        
        Args:
            result: åŒ…å«åœ°ç‚¹ä¿¡æ¯å’Œçˆ¬å–ç»“æœçš„å­—å…¸
        """
        parent_type = result.get("parent_type", "unknown")
        sub_type = result.get("sub_type", "unknown")
        place_id = result.get("place_id", "unknown")
        
        # è·å–å•†æˆ·åç§°ç”¨äºæ–‡ä»¶å‘½å
        business_name = result.get("business_info", {}).get("name", "")
        
        # è·å–è¾“å‡ºç›®å½•
        output_dir = self.get_output_path(parent_type, sub_type)
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        filename = self.generate_filename(place_id, business_name)
        output_file = output_dir / filename
        
        # ä¿å­˜ä¸ºç‹¬ç«‹çš„JSONæ–‡ä»¶ï¼ˆæ— éœ€é”ï¼Œå› ä¸ºæ¯ä¸ªæ–‡ä»¶éƒ½æ˜¯ç‹¬ç«‹çš„ï¼‰
        try:
            with open(output_file, 'w', encoding='utf-8') as f:  # ğŸ”¥ æ˜ç¡®ä½¿ç”¨UTF-8ç¼–ç 
                json.dump(result, f, ensure_ascii=False, indent=2)
                f.flush()  # ğŸ”¥ å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºåˆ°ç£ç›˜
                import os
                os.fsync(f.fileno())  # ğŸ”¥ å¼ºåˆ¶æ“ä½œç³»ç»Ÿç«‹å³å†™å…¥ç£ç›˜
            
            self.logger.debug(f"ç»“æœå·²å®æ—¶ä¿å­˜åˆ°: {output_file}")
            
            # ğŸ†• æ¸…ç†å¯¹åº”çš„CSVæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
            # å°è¯•å¤šç§æ–¹å¼è·å–å•†æˆ·åç§°ä»¥ä¾¿æ¸…ç†CSVæ–‡ä»¶
            self._cleanup_csv_file(result, business_name)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥ {output_file}: {e}")
            # ğŸ”¥ å°è¯•å¤‡ç”¨ä¿å­˜æ–¹å¼
            try:
                backup_dir = self.output_base_dir / "backup_results"
                backup_dir.mkdir(exist_ok=True)
                backup_file = backup_dir / f"backup_{place_id}.json"
                with open(backup_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                    f.flush()
                    import os
                    os.fsync(f.fileno())
                self.logger.warning(f"ç»“æœå·²ä¿å­˜åˆ°å¤‡ç”¨æ–‡ä»¶: {backup_file}")
            except Exception as backup_e:
                self.logger.error(f"å¤‡ç”¨ä¿å­˜ä¹Ÿå¤±è´¥: {backup_e}")
    
    def log_result(self, result: Dict[str, Any], is_success: bool):
        """
        è®°å½•å¤„ç†ç»“æœåˆ°ç›¸åº”çš„æ—¥å¿—æ–‡ä»¶
        
        åˆ†åˆ«è®°å½•æˆåŠŸå’Œå¤±è´¥çš„ç»“æœï¼Œä¾¿äºåç»­åˆ†æå’Œé—®é¢˜æ’æŸ¥
        ğŸ”¥ å¢å¼ºç‰ˆï¼šç¡®ä¿ç«‹å³åˆ·æ–°åˆ°ç£ç›˜ï¼Œé˜²æ­¢æ„å¤–ä¸­æ–­ä¸¢å¤±æ•°æ®
        
        Args:
            result: å¤„ç†ç»“æœå­—å…¸
            is_success: æ˜¯å¦æˆåŠŸ
        """
        log_file = self.success_log_file if is_success else self.error_log_file
        
        # æå–å…³é”®ä¿¡æ¯ç”¨äºæ—¥å¿—è®°å½•
        log_record = {
            "place_id": result.get("place_id"),
            "Maps_url": result.get("Maps_url"),
            "parent_type": result.get("parent_type"),
            "sub_type": result.get("sub_type"),
            "scrape_success": result.get("scrape_success"),
            "scrape_error_code": result.get("scrape_error_code"),
            "scrape_error_message": result.get("scrape_error_message"),
            "reviews_count": result.get("reviews_count", 0),
            "business_name": result.get("business_info", {}).get("name"),
            "retry_attempt": result.get("retry_attempt", 0),
            "timestamp": result.get("scraped_at"),
            "final_success": result.get("final_success", False)  # ğŸ†• æ·»åŠ final_successçŠ¶æ€
        }
        
        # çº¿ç¨‹å®‰å…¨åœ°å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼Œå¹¶ç«‹å³åˆ·æ–°åˆ°ç£ç›˜
        with self.lock:
            try:
                with open(log_file, 'a', encoding='utf-8') as f:  # ğŸ”¥ æ˜ç¡®ä½¿ç”¨UTF-8ç¼–ç 
                    f.write(json.dumps(log_record, ensure_ascii=False) + '\n')
                    f.flush()  # ğŸ”¥ å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒºåˆ°ç£ç›˜
                    import os
                    os.fsync(f.fileno())  # ğŸ”¥ å¼ºåˆ¶æ“ä½œç³»ç»Ÿç«‹å³å†™å…¥ç£ç›˜
                self.logger.debug(f"æ—¥å¿—å·²å®æ—¶ä¿å­˜: {result.get('place_id')} -> {log_file.name}")
            except Exception as e:
                self.logger.error(f"è®°å½•æ—¥å¿—å¤±è´¥: {e}")
                # ğŸ”¥ å°è¯•å¤‡ç”¨ä¿å­˜æ–¹å¼
                try:
                    backup_file = self.output_base_dir / f"backup_{log_file.name}"
                    with open(backup_file, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(log_record, ensure_ascii=False) + '\n')
                        f.flush()
                        import os
                        os.fsync(f.fileno())
                    self.logger.warning(f"æ—¥å¿—å·²ä¿å­˜åˆ°å¤‡ç”¨æ–‡ä»¶: {backup_file}")
                except Exception as backup_e:
                    self.logger.error(f"å¤‡ç”¨ä¿å­˜ä¹Ÿå¤±è´¥: {backup_e}")
    
    def process_batch(self, places: List[Dict[str, Any]], 
                     max_workers: int = 3, 
                     retry_failed: bool = False,
                     max_places: Optional[int] = None,
                     resume: bool = True, 
                     **kwargs):
        """
        æ‰¹é‡å¤„ç†åœ°ç‚¹æ•°æ®
        
        Args:
            places: åœ°ç‚¹æ•°æ®åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°
            retry_failed: æ˜¯å¦é‡è¯•å¤±è´¥çš„åœ°ç‚¹
            max_places: æ¯æ¬¡æœ€å¤§å¤„ç†æ•°é‡ï¼ˆNoneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
            resume: æ˜¯å¦å¯ç”¨æ–­ç‚¹æ¢å¤
            **kwargs: ä¼ é€’ç»™çˆ¬è™«çš„å‚æ•°
        """
        # åŠ è½½è¿›åº¦
        progress = self.load_progress()
        
        # è¿‡æ»¤éœ€è¦å¤„ç†çš„åœ°ç‚¹
        places_to_process, failed_place_ids = self.filter_places_to_process(places, retry_failed, max_places)
        
        if not places_to_process:
            self.logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„åœ°ç‚¹")
            return
        
        # åˆå§‹åŒ–æˆ–æ›´æ–°è¿›åº¦ä¿¡æ¯
        if not resume or progress["total"] == 0:
            progress = {
                "total": len(places_to_process),
                "current_index": 0,
                "successful": progress.get("successful", []),
                "failed": progress.get("failed", []),
                "skipped": progress.get("skipped", []),
                "success_count": len(progress.get("successful", [])),
                "failed_count": len(progress.get("failed", [])),
                "skipped_count": len(progress.get("skipped", [])),
                "start_time": time.strftime("%Y-%m-%d %H:%M:%S"),
                "session_info": {
                    "retry_mode": retry_failed,
                    "max_places": max_places,
                    "max_workers": max_workers
                }
            }
        else:
            # æ›´æ–°ä¼šè¯ä¿¡æ¯
            progress["session_info"].update({
                "retry_mode": retry_failed,
                "max_places": max_places,
                "max_workers": max_workers
            })
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total": len(places_to_process),
            "processed": 0,
            "success": 0,
            "failed": 0,
            "skipped": 0,
            "start_time": time.time()
        }
        
        self.logger.info(f"å¼€å§‹å¤„ç† {len(places_to_process)} ä¸ªåœ°ç‚¹")
        self.logger.info(f"é‡è¯•æ¨¡å¼: {'å¼€å¯' if retry_failed else 'å…³é—­'}")
        if max_places:
            self.logger.info(f"å¤„ç†æ•°é‡é™åˆ¶: {max_places}")
        
        # ä½¿ç”¨è¿›åº¦æ¡
        with tqdm(total=len(places_to_process), 
                 desc="æ‰¹é‡çˆ¬å–è¿›åº¦", unit="åœ°ç‚¹") as pbar:
            
            # ğŸ”¥ é‡æ–°è®¾è®¡çš„å¹¶å‘å¤„ç†é€»è¾‘
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                
                # ğŸ”¥ æ–¹æ³•1ï¼šç®€åŒ–çš„å¹¶å‘å¤„ç† - ä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ä»»åŠ¡
                if len(places_to_process) <= 500:  # ğŸ”¥ ä¿®æ”¹æ¡ä»¶ï¼Œ500ä¸ªä»¥ä¸‹éƒ½ä½¿ç”¨ç®€å•æ¨¡å¼
                    
                    # ä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ä»»åŠ¡
                    future_to_place = {}
                    for i, place_data in enumerate(places_to_process):
                        place_id = place_data["place_id"]
                        
                        # è®¾ç½®é‡è¯•å‚æ•°
                        is_retry = place_id in failed_place_ids
                        task_kwargs = kwargs.copy()
                        task_kwargs["retry_attempt"] = 1 if is_retry else 0
                        
                        future = executor.submit(self.scrape_single_place, place_data, **task_kwargs)
                        future_to_place[future] = (i, place_data)
                    
                    # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                    for future in as_completed(future_to_place):
                        index, place_data = future_to_place[future]
                        place_id = place_data["place_id"]
                        
                        try:
                            result = future.result()
                            
                            # ç«‹å³ä¿å­˜ç»“æœ
                            self.save_result(result)
                            
                            # åˆ¤æ–­æˆåŠŸå¤±è´¥å¹¶è®°å½•
                            if result.get("final_success", False):
                                stats["success"] += 1
                                if place_id not in progress["successful"]:
                                    progress["successful"].append(place_id)
                                # ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤ï¼ˆå¦‚æœæ˜¯é‡è¯•æˆåŠŸï¼‰
                                if place_id in progress["failed"]:
                                    progress["failed"].remove(place_id)
                                self.log_result(result, True)
                            else:
                                stats["failed"] += 1
                                if place_id not in progress["failed"]:
                                    progress["failed"].append(place_id)
                                self.log_result(result, False)
                            
                            stats["processed"] += 1
                            progress["current_index"] = index + 1
                            
                            # æ›´æ–°è¿›åº¦æ¡
                            pbar.update(1)
                            pbar.set_postfix({
                                "æˆåŠŸ": stats["success"],
                                "å¤±è´¥": stats["failed"],
                                "æˆåŠŸç‡": f"{stats['success']/(stats['success']+stats['failed'])*100:.1f}%" if (stats['success']+stats['failed']) > 0 else "0%"
                            })
                            
                            # å®šæœŸä¿å­˜è¿›åº¦
                            if stats["processed"] % self.config['auto_save_interval'] == 0:
                                progress["success_count"] = len(progress["successful"])
                                progress["failed_count"] = len(progress["failed"])
                                self.save_progress(progress)
                            
                        except Exception as e:
                            self.logger.error(f"å¤„ç†åœ°ç‚¹ {place_id} æ—¶å‡ºé”™: {e}")
                            stats["failed"] += 1
                            if place_id not in progress["failed"]:
                                progress["failed"].append(place_id)
                            
                            # è®°å½•å¼‚å¸¸åˆ°æ—¥å¿—
                            try:
                                exception_result = {
                                    **place_data,
                                    "scrape_success": False,
                                    "scrape_error_code": 1999,
                                    "scrape_error_message": f"å¤„ç†å¼‚å¸¸: {str(e)}",
                                    "business_info": {},
                                    "reviews_count": 0,
                                    "reviews": [],
                                    "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                                    "retry_attempt": 0,
                                    "has_reviews": None,
                                    "review_status": "å¤„ç†å¼‚å¸¸",
                                    "final_success": False
                                }
                                self.log_result(exception_result, False)
                            except Exception as log_e:
                                self.logger.error(f"è®°å½•å¼‚å¸¸æ—¥å¿—ä¹Ÿå¤±è´¥äº†: {log_e}")
                            
                            stats["processed"] += 1
                            pbar.update(1)
                            pbar.set_postfix({
                                "æˆåŠŸ": stats["success"],
                                "å¤±è´¥": stats["failed"],
                                "æˆåŠŸç‡": f"{stats['success']/(stats['success']+stats['failed'])*100:.1f}%" if (stats['success']+stats['failed']) > 0 else "0%"
                            })
                
                else:
                    # ğŸ”¥ æ–¹æ³•2ï¼šæµå¼å¤„ç† - é€‚åˆå¤§æ‰¹é‡æ•°æ®
                    
                    # ä½¿ç”¨é˜Ÿåˆ—ç®¡ç†ä»»åŠ¡
                    from collections import deque
                    remaining_places = deque(enumerate(places_to_process))
                    active_futures = {}
                    
                    # æäº¤åˆå§‹æ‰¹æ¬¡ä»»åŠ¡
                    while len(active_futures) < max_workers and remaining_places:
                        index, place_data = remaining_places.popleft()
                        place_id = place_data["place_id"]
                        
                        is_retry = place_id in failed_place_ids
                        task_kwargs = kwargs.copy()
                        task_kwargs["retry_attempt"] = 1 if is_retry else 0
                        
                        future = executor.submit(self.scrape_single_place, place_data, **task_kwargs)
                        active_futures[future] = (index, place_data)
                    
                    # æŒç»­å¤„ç†ç›´åˆ°æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    while active_futures:
                        # ç­‰å¾…è‡³å°‘ä¸€ä¸ªä»»åŠ¡å®Œæˆ
                        completed_futures = []
                        try:
                            # ğŸ”¥ ä¿®å¤ï¼šç§»é™¤è¶…æ—¶é™åˆ¶ï¼Œç­‰å¾…ä»»åŠ¡è‡ªç„¶å®Œæˆ
                            for future in as_completed(active_futures.keys()):
                                completed_futures.append(future)
                                break  # åªå¤„ç†ä¸€ä¸ªï¼Œä¿æŒæµå¼å¤„ç†
                        except Exception as e:
                            self.logger.error(f"ç­‰å¾…ä»»åŠ¡å®Œæˆæ—¶å‡ºé”™: {e}")
                            # å¦‚æœå‡ºé”™ï¼Œç­‰å¾…æ‰€æœ‰å‰©ä½™ä»»åŠ¡å®Œæˆ
                            for future in list(active_futures.keys()):
                                try:
                                    future.result(timeout=self.config.get('timeout', 300))
                                except Exception as future_e:
                                    self.logger.error(f"å¼ºåˆ¶ç­‰å¾…ä»»åŠ¡å®Œæˆå¤±è´¥: {future_e}")
                            break
                        
                        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                        for future in completed_futures:
                            index, place_data = active_futures.pop(future)
                            place_id = place_data["place_id"]
                            
                            try:
                                result = future.result()
                                
                                # ç«‹å³ä¿å­˜ç»“æœ
                                self.save_result(result)
                                
                                # åˆ¤æ–­æˆåŠŸå¤±è´¥å¹¶è®°å½•
                                if result.get("final_success", False):
                                    stats["success"] += 1
                                    if place_id not in progress["successful"]:
                                        progress["successful"].append(place_id)
                                    if place_id in progress["failed"]:
                                        progress["failed"].remove(place_id)
                                    self.log_result(result, True)
                                else:
                                    stats["failed"] += 1
                                    if place_id not in progress["failed"]:
                                        progress["failed"].append(place_id)
                                    self.log_result(result, False)
                                
                                stats["processed"] += 1
                                progress["current_index"] = index + 1
                                
                                # æ›´æ–°è¿›åº¦æ¡
                                pbar.update(1)
                                pbar.set_postfix({
                                    "æˆåŠŸ": stats["success"],
                                    "å¤±è´¥": stats["failed"],
                                    "æˆåŠŸç‡": f"{stats['success']/(stats['success']+stats['failed'])*100:.1f}%" if (stats['success']+stats['failed']) > 0 else "0%"
                                })
                                
                                # å®šæœŸä¿å­˜è¿›åº¦
                                if stats["processed"] % self.config['auto_save_interval'] == 0:
                                    progress["success_count"] = len(progress["successful"])
                                    progress["failed_count"] = len(progress["failed"])
                                    self.save_progress(progress)
                                
                            except Exception as e:
                                self.logger.error(f"å¤„ç†åœ°ç‚¹ {place_id} æ—¶å‡ºé”™: {e}")
                                stats["failed"] += 1
                                if place_id not in progress["failed"]:
                                    progress["failed"].append(place_id)
                                
                                # è®°å½•å¼‚å¸¸åˆ°æ—¥å¿—
                                try:
                                    exception_result = {
                                        **place_data,
                                        "scrape_success": False,
                                        "scrape_error_code": 1999,
                                        "scrape_error_message": f"å¤„ç†å¼‚å¸¸: {str(e)}",
                                        "business_info": {},
                                        "reviews_count": 0,
                                        "reviews": [],
                                        "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                                        "retry_attempt": 0,
                                        "has_reviews": None,
                                        "review_status": "å¤„ç†å¼‚å¸¸",
                                        "final_success": False
                                    }
                                    self.log_result(exception_result, False)
                                except Exception as log_e:
                                    self.logger.error(f"è®°å½•å¼‚å¸¸æ—¥å¿—ä¹Ÿå¤±è´¥äº†: {log_e}")
                                
                                stats["processed"] += 1
                                pbar.update(1)
                                pbar.set_postfix({
                                    "æˆåŠŸ": stats["success"],
                                    "å¤±è´¥": stats["failed"],
                                    "æˆåŠŸç‡": f"{stats['success']/(stats['success']+stats['failed'])*100:.1f}%" if (stats['success']+stats['failed']) > 0 else "0%"
                                })
                        
                        # æäº¤æ–°ä»»åŠ¡ä»¥ä¿æŒå¹¶å‘æ•°
                        while len(active_futures) < max_workers and remaining_places:
                            index, place_data = remaining_places.popleft()
                            place_id = place_data["place_id"]
                            
                            is_retry = place_id in failed_place_ids
                            task_kwargs = kwargs.copy()
                            task_kwargs["retry_attempt"] = 1 if is_retry else 0
                            
                            future = executor.submit(self.scrape_single_place, place_data, **task_kwargs)
                            active_futures[future] = (index, place_data)
        
        # ä¿å­˜æœ€ç»ˆè¿›åº¦
        self.save_progress(progress)
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        duration = time.time() - stats["start_time"]
        total_successful = len(progress["successful"])
        total_failed = len(progress["failed"])
        total_processed = total_successful + total_failed + len(progress["skipped"])
        
        self.logger.info(f"""
========================================
æœ¬æ¬¡å¤„ç†å®Œæˆï¼
========================================
æœ¬æ¬¡å¤„ç†åœ°ç‚¹: {stats['total']}
æœ¬æ¬¡æˆåŠŸ: {stats['success']}
æœ¬æ¬¡å¤±è´¥: {stats['failed']}
æœ¬æ¬¡è·³è¿‡: {stats['skipped']}
æœ¬æ¬¡æˆåŠŸç‡: {stats['success']/(stats['success']+stats['failed'])*100:.2f}% (å¦‚æœæœ‰å¤„ç†çš„è¯)

ç´¯è®¡ç»Ÿè®¡:
ç´¯è®¡æˆåŠŸ: {total_successful}
ç´¯è®¡å¤±è´¥: {total_failed}  
ç´¯è®¡å¤„ç†: {total_processed}
ç´¯è®¡æˆåŠŸç‡: {total_successful/(total_successful+total_failed)*100:.2f}% (å¦‚æœæœ‰å¤„ç†çš„è¯)

æ€§èƒ½ç»Ÿè®¡:
æ€»è€—æ—¶: {duration/3600:.2f} å°æ—¶
å¹³å‡é€Ÿåº¦: {stats['processed']/duration*60:.1f} åœ°ç‚¹/åˆ†é’Ÿ (å¦‚æœæœ‰å¤„ç†çš„è¯)
========================================
        """)
    
    def generate_summary_report(self):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        summary = {
            "total_places": 0,
            "successful_scrapes": 0,
            "failed_scrapes": 0,
            "by_type": {},
            "error_statistics": {},
            "review_statistics": {                    # ğŸ†• è¯„è®ºçŠ¶æ€ç»Ÿè®¡
                "has_reviews": 0,                     # æœ‰è¯„è®ºçš„åœ°ç‚¹æ•°
                "no_reviews": 0,                      # æ— è¯„è®ºçš„åœ°ç‚¹æ•°
                "no_review_function": 0,              # æ— è¯„è®ºåŠŸèƒ½çš„åœ°ç‚¹æ•°
                "failed_scrapes": 0,                  # æŠ“å–å¤±è´¥çš„åœ°ç‚¹æ•°
                "total_reviews_collected": 0          # æ”¶é›†åˆ°çš„æ€»è¯„è®ºæ•°
            },
            "progress_info": self.load_progress(),
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        # éå†æ‰€æœ‰è¾“å‡ºç›®å½•
        for parent_dir in self.output_base_dir.iterdir():
            if not parent_dir.is_dir() or parent_dir.name in ['progress.json', 'errors.jsonl', 'success.jsonl', 'batch_scraper.log']:
                continue
                
            parent_type = parent_dir.name
            if parent_type not in summary["by_type"]:
                summary["by_type"][parent_type] = {}
            
            for sub_dir in parent_dir.iterdir():
                if not sub_dir.is_dir():
                    continue
                    
                sub_type = sub_dir.name
                
                # ç»Ÿè®¡è¯¥ç±»å‹ä¸‹çš„æ‰€æœ‰JSONæ–‡ä»¶
                type_stats = {"total": 0, "success": 0, "failed": 0, "reviews_total": 0}
                
                # éå†æ‰€æœ‰JSONæ–‡ä»¶
                for json_file in sub_dir.glob("*.json"):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            result = json.load(f)
                            
                        type_stats["total"] += 1
                        
                        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨final_successè€Œä¸æ˜¯scrape_successè¿›è¡ŒçœŸæ­£çš„æˆåŠŸåˆ¤æ–­
                        if result.get("final_success", False):
                            type_stats["success"] += 1
                            type_stats["reviews_total"] += result.get("reviews_count", 0)
                            
                            # ğŸ†• ç»Ÿè®¡è¯„è®ºçŠ¶æ€
                            review_status = result.get("review_status", "")
                            if result.get("has_reviews"):
                                summary["review_statistics"]["has_reviews"] += 1
                                summary["review_statistics"]["total_reviews_collected"] += result.get("reviews_count", 0)
                            elif review_status == "æ— è¯„è®ºåŠŸèƒ½":
                                summary["review_statistics"]["no_review_function"] += 1
                            elif review_status == "æš‚æ— è¯„è®º":
                                summary["review_statistics"]["no_reviews"] += 1
                        else:
                            type_stats["failed"] += 1
                            summary["review_statistics"]["failed_scrapes"] += 1
                            error_code = result.get("scrape_error_code", "unknown")
                            summary["error_statistics"][error_code] = summary["error_statistics"].get(error_code, 0) + 1
                                
                    except (json.JSONDecodeError, FileNotFoundError) as e:
                        self.logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {json_file}: {e}")
                        continue
                
                if type_stats["total"] > 0:
                    summary["by_type"][parent_type][sub_type] = type_stats
                    summary["total_places"] += type_stats["total"]
                    summary["successful_scrapes"] += type_stats["success"]
                    summary["failed_scrapes"] += type_stats["failed"]
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary_file = self.output_base_dir / "summary_report.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {summary_file}")
        
        # è¾“å‡ºæ±‡æ€»ä¿¡æ¯åˆ°æ—¥å¿—
        self.logger.info(f"""
æ–‡ä»¶ç»“æ„æ±‡æ€»:
========================================
æ€»åœ°ç‚¹æ•°: {summary['total_places']}
æˆåŠŸå¤„ç†: {summary['successful_scrapes']}
å¤±è´¥å¤„ç†: {summary['failed_scrapes']}
æˆåŠŸç‡: {summary['successful_scrapes']/(summary['successful_scrapes']+summary['failed_scrapes'])*100:.2f}% (å¦‚æœæœ‰å¤„ç†çš„è¯)

è¯„è®ºç»Ÿè®¡:
æœ‰è¯„è®ºåœ°ç‚¹: {summary['review_statistics']['has_reviews']}
æ— è¯„è®ºåœ°ç‚¹: {summary['review_statistics']['no_reviews']}
æ— è¯„è®ºåŠŸèƒ½: {summary['review_statistics']['no_review_function']}
æŠ“å–å¤±è´¥: {summary['review_statistics']['failed_scrapes']}
æ€»è¯„è®ºæ•°: {summary['review_statistics']['total_reviews_collected']}

æŒ‰ç±»å‹åˆ†å¸ƒ:""")
        
        for parent_type, sub_types in summary["by_type"].items():
            self.logger.info(f"{parent_type}:")
            for sub_type, stats in sub_types.items():
                self.logger.info(f"  {sub_type}: {stats['success']}/{stats['total']} æˆåŠŸï¼Œ{stats['reviews_total']} æ¡è¯„è®º")
        
        return summary

    def unattended_processing(self, places: List[Dict[str, Any]], **kwargs):
        """
        æ— äººå€¼å®ˆå¤„ç†æ¨¡å¼
        
        é€‚åˆå¤§è§„æ¨¡æ•°æ®çš„é•¿æ—¶é—´è‡ªåŠ¨å¤„ç†ï¼ŒåŒ…å«è‡ªåŠ¨é‡è¯•å’ŒæŒç»­ç›‘æ§
        æ”¯æŒæ— é™è¿è¡Œæ¨¡å¼ï¼ˆç›´åˆ°æ‰‹åŠ¨åœæ­¢ï¼‰
        
        Args:
            places: åœ°ç‚¹æ•°æ®åˆ—è¡¨
            **kwargs: å…¶ä»–å¤„ç†å‚æ•°
        """
        max_rounds = self.config.get('max_continuous_rounds', 5)
        rest_time = self.config.get('rest_between_rounds', 300)
        auto_retry = self.config.get('auto_retry_failed', True)
        infinite_mode = self.config.get('infinite_mode', False)
        max_idle_rounds = self.config.get('max_idle_rounds', 3)
        checkpoint_interval = self.config.get('checkpoint_interval', 60) * 60  # è½¬æ¢ä¸ºç§’
        
        self.logger.info(f"å¼€å§‹æ— äººå€¼å®ˆå¤„ç†æ¨¡å¼")
        if infinite_mode:
            self.logger.info(f"ğŸ”„ æ— é™è¿è¡Œæ¨¡å¼å·²å¯ç”¨ - å°†æŒç»­è¿è¡Œç›´åˆ°æ‰‹åŠ¨åœæ­¢æˆ–æ•°æ®å¤„ç†å®Œæˆ")
            self.logger.info(f"ğŸ’¡ ä½¿ç”¨ Ctrl+C ä¼˜é›…åœæ­¢")
        else:
            self.logger.info(f"æœ€å¤§å¤„ç†è½®æ•°: {max_rounds}")
        self.logger.info(f"è½®æ¬¡é—´ä¼‘æ¯: {rest_time}ç§’")
        self.logger.info(f"è‡ªåŠ¨é‡è¯•å¤±è´¥: {auto_retry}")
        self.logger.info(f"æœ€å¤§ç©ºé—²è½®æ•°: {max_idle_rounds}")
        
        round_num = 0
        idle_rounds = 0
        last_checkpoint = time.time()
        
        try:
            # ğŸ”¥ æ— é™æ¨¡å¼æˆ–æŒ‡å®šè½®æ•°æ¨¡å¼
            while infinite_mode or round_num < max_rounds:
                round_num += 1
                self.logger.info(f"\n=== ç¬¬ {round_num} è½®å¤„ç†å¼€å§‹ ===")
                
                # æ£€æŸ¥è¿˜æœ‰å¤šå°‘åœ°ç‚¹éœ€è¦å¤„ç†
                remaining_places = self.filter_places_to_process(
                    places, 
                    retry_failed=False, 
                    max_places=self.config.get('max_places')
                )
                
                if not remaining_places:
                    idle_rounds += 1
                    self.logger.info(f"æ— æ–°åœ°ç‚¹éœ€è¦å¤„ç†ï¼ˆç©ºé—²è½®æ•°: {idle_rounds}/{max_idle_rounds}ï¼‰")
                    
                    if idle_rounds >= max_idle_rounds:
                        self.logger.info("è¾¾åˆ°æœ€å¤§ç©ºé—²è½®æ•°ï¼Œæ‰€æœ‰åœ°ç‚¹å·²å¤„ç†å®Œæˆï¼")
                        break
                    
                    # ç©ºé—²æ—¶ä¼‘æ¯æ›´çŸ­æ—¶é—´
                    idle_rest_time = min(rest_time, 60)
                    self.logger.info(f"ç­‰å¾… {idle_rest_time} ç§’åç»§ç»­æ£€æŸ¥...")
                    time.sleep(idle_rest_time)
                    continue
                else:
                    # æœ‰æ–°åœ°ç‚¹å¤„ç†ï¼Œé‡ç½®ç©ºé—²è®¡æ•°
                    idle_rounds = 0
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                try:
                    batch_start_time = time.time()
                    
                    self.process_batch(
                        places,
                        max_workers=self.config['max_workers'],
                        retry_failed=False,
                        max_places=self.config.get('max_places'),
                        resume=True,
                        **kwargs
                    )
                    
                    batch_duration = time.time() - batch_start_time
                    
                    # å¤„ç†å®Œæˆåï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨é‡è¯•å¤±è´¥åœ°ç‚¹
                    if auto_retry:
                        progress = self.load_progress()
                        failed_count = len(progress.get('failed', []))
                        
                        if failed_count > 0:
                            self.logger.info(f"å¼€å§‹è‡ªåŠ¨é‡è¯• {failed_count} ä¸ªå¤±è´¥åœ°ç‚¹")
                            # åˆ›å»ºé‡è¯•å‚æ•°ï¼Œé¿å…å‚æ•°å†²çª
                            retry_kwargs = kwargs.copy()
                            retry_kwargs.update({
                                'max_retries': self.config['max_retries'],
                                'timeout': self.config['timeout'] * 1.5,
                            })
                            self.process_batch(
                                places,
                                max_workers=max(1, self.config['max_workers'] - 1),  # é™ä½å¹¶å‘
                                retry_failed=True,
                                resume=True,
                                **retry_kwargs
                            )
                    
                    # ç”Ÿæˆå½“å‰è½®æ¬¡æŠ¥å‘Š
                    summary = self.generate_summary_report()
                    success_rate = summary['successful_scrapes'] / (summary['successful_scrapes'] + summary['failed_scrapes']) * 100 if (summary['successful_scrapes'] + summary['failed_scrapes']) > 0 else 0
                    
                    self.logger.info(f"ç¬¬ {round_num} è½®å®Œæˆï¼ŒæˆåŠŸç‡: {success_rate:.1f}%ï¼Œè€—æ—¶: {batch_duration/60:.1f}åˆ†é’Ÿ")
                    
                    # æ£€æŸ¥ç‚¹ä¿¡æ¯è¾“å‡º
                    current_time = time.time()
                    if current_time - last_checkpoint >= checkpoint_interval:
                        self.logger.info(f"""
ğŸ“Š æ£€æŸ¥ç‚¹æŠ¥å‘Š (ç¬¬ {round_num} è½®):
========================================
æ€»æˆåŠŸ: {summary['successful_scrapes']}
æ€»å¤±è´¥: {summary['failed_scrapes']}
å½“å‰æˆåŠŸç‡: {success_rate:.1f}%
æ€»è¯„è®ºæ•°: {summary['review_statistics']['total_reviews_collected']}
è¿è¡Œæ¨¡å¼: {'æ— é™æ¨¡å¼' if infinite_mode else f'{max_rounds}è½®æ¨¡å¼'}
ç©ºé—²è½®æ•°: {idle_rounds}/{max_idle_rounds}
========================================
                        """)
                        last_checkpoint = current_time
                    
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¤±è´¥éœ€è¦å¤„ç†ï¼ˆä»…åœ¨éæ— é™æ¨¡å¼ä¸‹æ£€æŸ¥ï¼‰
                    if not infinite_mode:
                        progress = self.load_progress()
                        remaining_failed = len(progress.get('failed', []))
                        
                        if remaining_failed == 0:
                            self.logger.info("æ‰€æœ‰åœ°ç‚¹å¤„ç†æˆåŠŸï¼Œæ— äººå€¼å®ˆæ¨¡å¼å®Œæˆï¼")
                            break
                    
                    # è½®æ¬¡é—´ä¼‘æ¯ï¼ˆé™¤äº†æœ€åä¸€è½®æˆ–æ— é™æ¨¡å¼ï¼‰
                    if (not infinite_mode and round_num < max_rounds) or infinite_mode:
                        self.logger.info(f"ç¬¬ {round_num} è½®å®Œæˆï¼Œä¼‘æ¯ {rest_time} ç§’åç»§ç»­...")
                        time.sleep(rest_time)
                        
                except Exception as e:
                    self.logger.error(f"ç¬¬ {round_num} è½®å¤„ç†å‡ºé”™: {e}")
                    if infinite_mode or round_num < max_rounds:
                        self.logger.info(f"ä¼‘æ¯ {rest_time} ç§’åé‡è¯•...")
                        time.sleep(rest_time)
                    else:
                        self.logger.error("å·²è¾¾åˆ°æœ€å¤§è½®æ•°ï¼Œæ— äººå€¼å®ˆæ¨¡å¼ç»“æŸ")
                        break
                
        except KeyboardInterrupt:
            self.logger.info("\nğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…åœæ­¢...")
            self.logger.info(f"âœ… å·²å®Œæˆ {round_num} è½®å¤„ç†")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_summary = self.generate_summary_report()
        self.logger.info(f"""
========================================
æ— äººå€¼å®ˆå¤„ç†å®Œæˆï¼
========================================
è¿è¡Œæ¨¡å¼: {'æ— é™æ¨¡å¼' if infinite_mode else f'{max_rounds}è½®æ¨¡å¼'}
å®é™…å¤„ç†è½®æ•°: {round_num}
æœ€ç»ˆæˆåŠŸ: {final_summary['successful_scrapes']}
æœ€ç»ˆå¤±è´¥: {final_summary['failed_scrapes']}
æœ€ç»ˆæˆåŠŸç‡: {final_summary['successful_scrapes']/(final_summary['successful_scrapes']+final_summary['failed_scrapes'])*100:.2f}% (å¦‚æœæœ‰å¤„ç†çš„è¯)
æ€»è¯„è®ºæ•°: {final_summary['review_statistics']['total_reviews_collected']}
========================================
        """)
        
        return final_summary

def main():
    parser = argparse.ArgumentParser(description='å¤§è§„æ¨¡æ‰¹é‡Google Mapsæ•°æ®çˆ¬å–')
    parser.add_argument('input_file', nargs='?', help='JSONLæ ¼å¼çš„è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--output-dir', default='batch_output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-workers', type=int, help='æœ€å¤§å¹¶å‘æ•°')
    parser.add_argument('--max-retries', type=int, default=3, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    parser.add_argument('--timeout', type=int, default=300, help='å•ä¸ªåœ°ç‚¹è¶…æ—¶æ—¶é—´(ç§’)')
    parser.add_argument('--no-resume', action='store_true', help='ä¸ä»ä¸Šæ¬¡ä¸­æ–­å¤„ç»§ç»­')
    parser.add_argument('--retry-failed', action='store_true', help='é‡è¯•å¤±è´¥çš„åœ°ç‚¹')
    parser.add_argument('--max-places', type=int, help='æ¯æ¬¡æœ€å¤§å¤„ç†åœ°ç‚¹æ•°é‡')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†æ—¥å¿—')
    parser.add_argument('--generate-report', action='store_true', help='åªç”Ÿæˆæ±‡æ€»æŠ¥å‘Š')
    parser.add_argument('--show-status', action='store_true', help='æ˜¾ç¤ºå½“å‰å¤„ç†çŠ¶æ€')
    
    # ğŸ†• æ— äººå€¼å®ˆæ¨¡å¼å‚æ•°
    parser.add_argument('--unattended', action='store_true', help='æ— äººå€¼å®ˆæ¨¡å¼ï¼ˆé€‚åˆå¤§è§„æ¨¡æ•°æ®é•¿æ—¶é—´å¤„ç†ï¼‰')
    parser.add_argument('--infinite', action='store_true', help='ğŸ”„ æ— é™è¿è¡Œæ¨¡å¼ï¼ˆæŒç»­è¿è¡Œç›´åˆ°æ‰‹åŠ¨åœæ­¢ï¼‰')
    parser.add_argument('--fast', action='store_true', help='âš¡ å¿«é€Ÿæ¨¡å¼ï¼ˆé€‚åˆæµ‹è¯•æˆ–å°è§„æ¨¡æ•°æ®ï¼‰')
    parser.add_argument('--max-rounds', type=int, default=5, help='æ— äººå€¼å®ˆæ¨¡å¼çš„æœ€å¤§å¤„ç†è½®æ•°')
    parser.add_argument('--rest-time', type=int, help='è½®æ¬¡é—´ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰')
    parser.add_argument('--checkpoint-interval', type=int, help='æ£€æŸ¥ç‚¹é—´éš”æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰')
    
    args = parser.parse_args()
    
    # ğŸ”¥ æ ¹æ®è¿è¡Œæ¨¡å¼é€‰æ‹©é…ç½®
    config = None
    if args.infinite:
        # æ— é™è¿è¡Œæ¨¡å¼
        config = INFINITE_CONFIG.copy()
        print("ğŸ”„ ä½¿ç”¨æ— é™è¿è¡Œæ¨¡å¼é…ç½®")
    elif args.fast:
        # å¿«é€Ÿæ¨¡å¼
        config = FAST_CONFIG.copy()
        print("âš¡ ä½¿ç”¨å¿«é€Ÿæ¨¡å¼é…ç½®")
    elif args.unattended:
        # ä¼ ç»Ÿæ— äººå€¼å®ˆæ¨¡å¼
        config = UNATTENDED_CONFIG.copy()
        print("ğŸ”§ ä½¿ç”¨æ— äººå€¼å®ˆæ¨¡å¼é…ç½®")
    
    # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if config:
        if args.max_places:
            config['max_places'] = args.max_places
        if args.max_workers is not None:  # ğŸ”¥ åªæœ‰æ˜ç¡®æŒ‡å®šæ—¶æ‰è¦†ç›–
            config['max_workers'] = args.max_workers
        if args.max_rounds:
            config['max_continuous_rounds'] = args.max_rounds
        if args.rest_time:
            config['rest_between_rounds'] = args.rest_time
        if args.checkpoint_interval:
            config['checkpoint_interval'] = args.checkpoint_interval
    
    scraper = BatchGoogleMapsScraper(output_base_dir=args.output_dir, config=config)
    
    if args.generate_report:
        # åªç”ŸæˆæŠ¥å‘Š
        summary = scraper.generate_summary_report()
        print(json.dumps(summary, ensure_ascii=False, indent=2))
        return
    
    if args.show_status:
        # æ˜¾ç¤ºçŠ¶æ€
        progress = scraper.load_progress()
        print(json.dumps(progress, ensure_ascii=False, indent=2))
        return
    
    if not args.input_file:
        parser.print_help()
        return
    
    # åŠ è½½è¾“å…¥æ•°æ®
    places = scraper.load_jsonl_input(args.input_file)
    if not places:
        print("æ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆçš„åœ°ç‚¹æ•°æ®")
        return
    
    # ğŸ†• æ ¹æ®æ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
    if args.infinite or args.unattended:
        # æ— äººå€¼å®ˆæ¨¡å¼ï¼ˆåŒ…æ‹¬æ— é™æ¨¡å¼ï¼‰
        mode_name = "æ— é™è¿è¡Œ" if args.infinite else "æ— äººå€¼å®ˆ"
        print(f"å¼€å§‹{mode_name}å¤„ç† {len(places)} ä¸ªåœ°ç‚¹...")
        if args.infinite:
            print("ğŸ’¡ ä½¿ç”¨ Ctrl+C ä¼˜é›…åœæ­¢")
        else:
            max_rounds = config.get('max_continuous_rounds', 5) if config else 5
            print(f"æœ€å¤šè¿è¡Œ {max_rounds} è½®")
        
        scraper.unattended_processing(
            places,
            max_retries=args.max_retries,
            timeout=args.timeout,
            verbose=args.verbose
        )
    else:
        # æ ‡å‡†æ‰¹é‡å¤„ç†æ¨¡å¼
        # ğŸ”¥ ä½¿ç”¨é…ç½®ä¸­çš„max_workersï¼Œå¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šçš„è¯
        max_workers = args.max_workers if args.max_workers is not None else 3
        scraper.process_batch(
            places,
            max_workers=max_workers,
            retry_failed=args.retry_failed,
            max_places=args.max_places,
            resume=not args.no_resume,
            max_retries=args.max_retries,
            timeout=args.timeout,
            verbose=args.verbose
        )
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    scraper.generate_summary_report()

if __name__ == "__main__":
    main() 