#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¾“å…¥æ–‡ä»¶ç»Ÿè®¡å·¥å…·
ç»Ÿè®¡JSONLæ–‡ä»¶ä¸­æ¯ä¸ªparent_typeå’Œsub_typeçš„åœ°ç‚¹æ•°é‡

ç”¨æ³•ï¼š
python ç»Ÿè®¡è¾“å…¥æ–‡ä»¶.py [è¾“å…¥æ–‡ä»¶è·¯å¾„]

ä½œè€…ï¼šAI Assistant
"""

import json
import sys
from pathlib import Path
from collections import defaultdict
import time

def load_jsonl_with_encoding(file_path):
    """
    åŠ è½½JSONLæ–‡ä»¶ï¼Œå°è¯•å¤šç§ç¼–ç æ–¹å¼
    
    Args:
        file_path: JSONLæ–‡ä»¶è·¯å¾„
        
    Returns:
        (æ•°æ®åˆ—è¡¨, ä½¿ç”¨çš„ç¼–ç )
    """
    encodings_to_try = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312']
    
    for encoding in encodings_to_try:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                file_content = f.read()
                return file_content, encoding
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨UTF-8å¹¶å¿½ç•¥é”™è¯¯
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        file_content = f.read()
        return file_content, 'utf-8 (with errors ignored)'

def analyze_places_data(input_file):
    """
    åˆ†æåœ°ç‚¹æ•°æ®å¹¶ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    
    Args:
        input_file: è¾“å…¥çš„JSONLæ–‡ä»¶è·¯å¾„
        
    Returns:
        ç»Ÿè®¡ç»“æœå­—å…¸
    """
    print(f"ğŸ“Š å¼€å§‹åˆ†ææ–‡ä»¶: {input_file}")
    
    # è¯»å–æ–‡ä»¶
    try:
        file_content, used_encoding = load_jsonl_with_encoding(input_file)
        print(f"âœ… ä½¿ç”¨ç¼–ç : {used_encoding}")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None
    
    # ç»Ÿè®¡æ•°æ®ç»“æ„
    stats = {
        "total_places": 0,
        "parent_types": defaultdict(int),  # parent_type -> æ€»æ•°
        "sub_types": defaultdict(int),     # sub_type -> æ€»æ•°
        "type_combinations": defaultdict(int),  # parent_type/sub_type -> æ•°é‡
        "detailed_breakdown": defaultdict(lambda: defaultdict(int)),  # parent_type -> {sub_type: count}
        "missing_fields": {
            "missing_parent_type": 0,
            "missing_sub_type": 0,
            "missing_place_id": 0,
            "missing_maps_url": 0
        }
    }
    
    # é€è¡Œè§£æJSONL
    line_count = 0
    valid_places = 0
    
    for line_num, line in enumerate(file_content.splitlines(), 1):
        line = line.strip()
        if not line:
            continue
            
        line_count += 1
        
        try:
            place_data = json.loads(line)
            
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            place_id = place_data.get('place_id', '')
            maps_url = place_data.get('Maps_url', '')
            parent_type = place_data.get('parent_type', '')
            sub_type = place_data.get('sub_type', '')
            
            # ç»Ÿè®¡ç¼ºå¤±å­—æ®µ
            if not place_id:
                stats["missing_fields"]["missing_place_id"] += 1
            if not maps_url:
                stats["missing_fields"]["missing_maps_url"] += 1
            if not parent_type:
                stats["missing_fields"]["missing_parent_type"] += 1
            if not sub_type:
                stats["missing_fields"]["missing_sub_type"] += 1
            
            # åªç»Ÿè®¡æœ‰æœ‰æ•ˆparent_typeå’Œsub_typeçš„è®°å½•
            if parent_type and sub_type:
                valid_places += 1
                stats["parent_types"][parent_type] += 1
                stats["sub_types"][sub_type] += 1
                stats["type_combinations"][f"{parent_type}/{sub_type}"] += 1
                stats["detailed_breakdown"][parent_type][sub_type] += 1
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸  ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
            continue
    
    stats["total_places"] = valid_places
    stats["total_lines"] = line_count
    
    print(f"âœ… è§£æå®Œæˆï¼")
    print(f"   æ€»è¡Œæ•°: {line_count}")
    print(f"   æœ‰æ•ˆåœ°ç‚¹: {valid_places}")
    print(f"   Parentç±»å‹æ•°: {len(stats['parent_types'])}")
    print(f"   Subç±»å‹æ•°: {len(stats['sub_types'])}")
    print(f"   ç±»å‹ç»„åˆæ•°: {len(stats['type_combinations'])}")
    
    return stats

def generate_report(stats, output_file):
    """
    ç”Ÿæˆè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘Š
    
    Args:
        stats: ç»Ÿè®¡æ•°æ®
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # å‡†å¤‡æŠ¥å‘Šæ•°æ®
    report = {
        "analysis_time": time.strftime("%Y-%m-%d %H:%M:%S"),
        "summary": {
            "total_places": stats["total_places"],
            "total_lines": stats["total_lines"],
            "unique_parent_types": len(stats["parent_types"]),
            "unique_sub_types": len(stats["sub_types"]),
            "unique_combinations": len(stats["type_combinations"])
        },
        "data_quality": {
            "missing_fields": dict(stats["missing_fields"]),
            "data_completeness_rate": (stats["total_places"] / stats["total_lines"] * 100) if stats["total_lines"] > 0 else 0
        },
        "parent_type_distribution": dict(sorted(stats["parent_types"].items(), key=lambda x: x[1], reverse=True)),
        "sub_type_distribution": dict(sorted(stats["sub_types"].items(), key=lambda x: x[1], reverse=True)),
        "detailed_breakdown": {}
    }
    
    # æ•´ç†è¯¦ç»†åˆ†è§£æ•°æ®
    for parent_type, sub_types in stats["detailed_breakdown"].items():
        report["detailed_breakdown"][parent_type] = {
            "total": stats["parent_types"][parent_type],
            "sub_types": dict(sorted(sub_types.items(), key=lambda x: x[1], reverse=True))
        }
    
    # æŒ‰parent_typeçš„æ€»æ•°æ’åº
    report["detailed_breakdown"] = dict(sorted(
        report["detailed_breakdown"].items(), 
        key=lambda x: x[1]["total"], 
        reverse=True
    ))
    
    # ä¿å­˜æŠ¥å‘Šä¸ºJSONLæ ¼å¼ï¼ˆæ¯ä¸ªparent_typeä¸€è¡Œï¼Œä¾¿äºé˜…è¯»ï¼‰
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥æ±‡æ€»ä¿¡æ¯
            f.write(json.dumps({
                "type": "summary",
                "data": report["summary"]
            }, ensure_ascii=False) + '\n')
            
            f.write(json.dumps({
                "type": "data_quality",
                "data": report["data_quality"]
            }, ensure_ascii=False) + '\n')
            
            # å†™å…¥æ¯ä¸ªparent_typeçš„è¯¦ç»†ä¿¡æ¯
            for parent_type, breakdown in report["detailed_breakdown"].items():
                f.write(json.dumps({
                    "type": "parent_type_breakdown",
                    "parent_type": parent_type,
                    "data": breakdown
                }, ensure_ascii=False) + '\n')
            
            # å†™å…¥topå­ç±»å‹åˆ†å¸ƒ
            f.write(json.dumps({
                "type": "top_sub_types",
                "data": dict(list(report["sub_type_distribution"].items())[:20])  # å‰20ä¸ª
            }, ensure_ascii=False) + '\n')
        
        print(f"âœ… ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
        return False
    
    return True

def print_summary(stats):
    """
    åœ¨æ§åˆ¶å°æ‰“å°æ±‡æ€»ä¿¡æ¯
    
    Args:
        stats: ç»Ÿè®¡æ•°æ®
    """
    print(f"""
ğŸ“Š ç»Ÿè®¡æ±‡æ€»:
========================================
æ€»åœ°ç‚¹æ•°: {stats['total_places']:,}
Parentç±»å‹æ•°: {len(stats['parent_types'])}
Subç±»å‹æ•°: {len(stats['sub_types'])}
ç±»å‹ç»„åˆæ•°: {len(stats['type_combinations'])}

ğŸ” Top 10 Parentç±»å‹:""")
    
    # æ˜¾ç¤ºTop 10 parent types
    sorted_parents = sorted(stats['parent_types'].items(), key=lambda x: x[1], reverse=True)
    for i, (parent_type, count) in enumerate(sorted_parents[:10], 1):
        print(f"{i:2d}. {parent_type}: {count:,} ä¸ªåœ°ç‚¹")
    
    print(f"""
ğŸ” Top 10 Subç±»å‹:""")
    
    # æ˜¾ç¤ºTop 10 sub types  
    sorted_subs = sorted(stats['sub_types'].items(), key=lambda x: x[1], reverse=True)
    for i, (sub_type, count) in enumerate(sorted_subs[:10], 1):
        print(f"{i:2d}. {sub_type}: {count:,} ä¸ªåœ°ç‚¹")
    
    # æ•°æ®è´¨é‡ä¿¡æ¯
    if any(stats['missing_fields'].values()):
        print(f"""
âš ï¸  æ•°æ®è´¨é‡è­¦å‘Š:""")
        for field, count in stats['missing_fields'].items():
            if count > 0:
                print(f"   {field}: {count} æ¡è®°å½•")

def main():
    """ä¸»å‡½æ•°"""
    # å¤„ç†å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        input_file = sys.argv[1]
    else:
        input_file = "input/hong_kong_place_ids_and_urls_ENG.jsonl"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    input_path = Path(input_file)
    if not input_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        print(f"ğŸ’¡ ç”¨æ³•: python {Path(__file__).name} [è¾“å…¥æ–‡ä»¶è·¯å¾„]")
        return
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = input_path.parent / f"{input_path.stem}_statistics.jsonl"
    
    print(f"ğŸš€ å¼€å§‹ç»Ÿè®¡åˆ†æ...")
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # åˆ†ææ•°æ®
    stats = analyze_places_data(input_file)
    if not stats:
        return
    
    # ç”ŸæˆæŠ¥å‘Š
    if generate_report(stats, output_file):
        # æ‰“å°æ±‡æ€»ä¿¡æ¯
        print_summary(stats)
        print(f"""
========================================
ğŸ‰ ç»Ÿè®¡åˆ†æå®Œæˆï¼
ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}

ğŸ’¡ æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹æŠ¥å‘Šå†…å®¹:
   cat "{output_file}"
   æˆ–è€…ç”¨æ–‡æœ¬ç¼–è¾‘å™¨æ‰“å¼€æŸ¥çœ‹
========================================""")
    else:
        print("âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥")

if __name__ == "__main__":
    main() 