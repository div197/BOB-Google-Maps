[English Version](#google-maps-review-scraper)

# Google Maps è¯„è®ºçˆ¬è™«

ä¸€ä¸ªå¼ºå¤§çš„Google Mapså•†æˆ·ä¿¡æ¯å’Œç”¨æˆ·è¯„è®ºæ•°æ®çˆ¬å–å·¥å…·ï¼Œä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®é‡‡é›†è®¾è®¡ã€‚

## ğŸŒŸ ä¸»è¦ç‰¹æ€§

- **å¤§è§„æ¨¡æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒæ•°ä¸‡ä¸ªåœ°ç‚¹çš„æ‰¹é‡æ•°æ®çˆ¬å–
- **æ™ºèƒ½æ–­ç‚¹æ¢å¤**ï¼šæ„å¤–ä¸­æ–­åå¯è‡ªåŠ¨ç»§ç»­å¤„ç†
- **å¤šçº¿ç¨‹å¹¶å‘**ï¼šæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘å¤„ç†ï¼Œæé«˜æ•ˆç‡
- **é”™è¯¯é‡è¯•æœºåˆ¶**ï¼šæ™ºèƒ½é‡è¯•å¤±è´¥çš„åœ°ç‚¹ï¼Œæé«˜æˆåŠŸç‡
- **è¯¦ç»†è¿›åº¦è·Ÿè¸ª**ï¼šå®æ—¶æ˜¾ç¤ºå¤„ç†è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯
- **çµæ´»è¾“å‡ºæ ¼å¼**ï¼šæ”¯æŒJSONå’ŒCSVæ ¼å¼è¾“å‡º
- **å®Œæ•´æ•°æ®æå–**ï¼šåŒ…æ‹¬å•†æˆ·åŸºæœ¬ä¿¡æ¯ã€è¯„è®ºå†…å®¹ã€è¯„åˆ†ç­‰

## ğŸ“š æ–‡æ¡£å¯¼èˆª

- ğŸ“– **[æŠ€æœ¯æ–‡æ¡£](TECHNICAL.md)** - è¯¦ç»†çš„ä»£ç åŠŸèƒ½è¯´æ˜å’Œå®ç°ç»†èŠ‚
- ğŸ“‹ **[é”™è¯¯ä»£ç ](error_codes.md)** - å®Œæ•´çš„é”™è¯¯ä»£ç å®šä¹‰å’Œè§£å†³æ–¹æ¡ˆ

## ğŸ“ é¡¹ç›®ç»“æ„

```
googlemaps-scraper-reviews/
â”œâ”€â”€ main.py                    # å•ç‚¹çˆ¬è™«æ ¸å¿ƒè„šæœ¬
â”œâ”€â”€ batch_scraper.py          # æ‰¹é‡å¤„ç†è„šæœ¬
â”œâ”€â”€ process_urls.py           # URLé¢„å¤„ç†å·¥å…·ï¼Œå¼ºåˆ¶è‹±æ–‡ç•Œé¢
â”œâ”€â”€ check_progress.py         # è¿›åº¦æ£€æŸ¥å·¥å…·
â”œâ”€â”€ retry_failed_scrapes.py   # å¤±è´¥é‡è¯•å·¥å…·
â”œâ”€â”€ find_duplicates_simple.py # å»é‡å·¥å…·
â”œâ”€â”€ input_statistics.py       # è¾“å…¥æ•°æ®ç»Ÿè®¡
â”œâ”€â”€ error_codes.md           # é”™è¯¯ä»£ç è¯´æ˜
â”œâ”€â”€ input/                   # è¾“å…¥æ•°æ®ç›®å½•
â”œâ”€â”€ batch_output/           # æ‰¹é‡è¾“å‡ºæ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ culture/           # æ–‡åŒ–åœºæ‰€æ•°æ®
â”‚   â”œâ”€â”€ education/         # æ•™è‚²åœºæ‰€æ•°æ®
â”‚   â”œâ”€â”€ entertainment_and_recreation/ # å¨±ä¹ä¼‘é—²æ•°æ®
â”‚   â”œâ”€â”€ facilities/        # è®¾æ–½æ•°æ®
â”‚   â”œâ”€â”€ food_and_drink/    # é¤é¥®æ•°æ®
â”‚   â”œâ”€â”€ natural_features/  # è‡ªç„¶æ™¯è§‚æ•°æ®
â”‚   â”œâ”€â”€ places_of_worship/ # å®—æ•™åœºæ‰€æ•°æ®
â”‚   â”œâ”€â”€ shopping/          # è´­ç‰©åœºæ‰€æ•°æ®
â”‚   â”œâ”€â”€ sports/            # ä½“è‚²åœºæ‰€æ•°æ®
â”‚   â”œâ”€â”€ transportation/    # äº¤é€šç›¸å…³æ•°æ®
â”‚   â”œâ”€â”€ progress.json      # å¤„ç†è¿›åº¦è®°å½•
â”‚   â”œâ”€â”€ errors.jsonl       # é”™è¯¯æ—¥å¿—
â”‚   â””â”€â”€ success.jsonl      # æˆåŠŸæ—¥å¿—
â””â”€â”€ backup/                # å¤‡ä»½æ•°æ®ç›®å½•
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.7+
- Chromeæµè§ˆå™¨
- ChromeDriverï¼ˆä¼šè‡ªåŠ¨ç®¡ç†ï¼‰

### å®‰è£…ä¾èµ–

```bash
pip install selenium pandas tqdm
```

### å‡†å¤‡è¾“å…¥æ•°æ®

åœ¨ `input/` ç›®å½•ä¸‹æ”¾ç½®JSONLæ ¼å¼çš„è¾“å…¥æ–‡ä»¶ï¼Œæ¯è¡ŒåŒ…å«ä¸€ä¸ªåœ°ç‚¹çš„ä¿¡æ¯ï¼š

```json
{
    "place_id": "ChIJ...",
    "place_name": "å•†æˆ·åç§°",
    "parent_type": "food_and_drink",
    "sub_type": "restaurant",
    "url": "https://maps.google.com/..."
}
```

### URLé¢„å¤„ç†ï¼šå¼ºåˆ¶è‹±æ–‡ç•Œé¢

**é‡è¦æç¤ºï¼š** å½“å‰ç‰ˆæœ¬çš„çˆ¬è™«ä»…èƒ½è¯†åˆ«å’Œè§£æGoogle Mapsçš„ **è‹±æ–‡é¡µé¢** ç»“æ„ã€‚ä¸ºç¡®ä¿æ•°æ®èƒ½è¢«æ­£ç¡®æŠ“å–ï¼Œæ‰€æœ‰URLéƒ½å¿…é¡»ç»è¿‡é¢„å¤„ç†ä»¥å¼ºåˆ¶é¡µé¢ä»¥è‹±æ–‡æ˜¾ç¤ºã€‚æœªç»å¤„ç†çš„URLå°†å¯¼è‡´çˆ¬å–å¤±è´¥ã€‚

é¡¹ç›®ä¸­çš„ `process_urls.py` è„šæœ¬å¯ç”¨äºæ‰¹é‡ä¸ºURLæ·»åŠ  `&hl=en` å‚æ•°ã€‚

**ä½¿ç”¨æ­¥éª¤ï¼š**

1.  **é…ç½®è„šæœ¬**ï¼šæ‰“å¼€ `process_urls.py` æ–‡ä»¶ï¼Œæ ¹æ®æ‚¨çš„æ–‡ä»¶ä½ç½®ä¿®æ”¹ `input_file` å’Œ `output_file` å˜é‡ã€‚
2.  **è¿è¡Œè„šæœ¬**ï¼š
    ```bash
    python process_urls.py
    ```
3.  **ä½¿ç”¨æ–°æ–‡ä»¶**ï¼šè„šæœ¬ä¼šç”Ÿæˆä¸€ä¸ªå¤„ç†åçš„æ–°æ–‡ä»¶ã€‚è¯·åŠ¡å¿…ä½¿ç”¨è¿™ä¸ªæ–°ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆä¾‹å¦‚ `..._en.jsonl`ï¼‰ä½œä¸ºåç»­çˆ¬å–æ“ä½œçš„è¾“å…¥ã€‚

### å•ç‚¹çˆ¬å–

çˆ¬å–å•ä¸ªåœ°ç‚¹çš„æ•°æ®ï¼š

```bash
python main.py "https://maps.google.com/..."
```

æ”¯æŒçš„å‚æ•°ï¼š
- `--output-dir`ï¼šè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼šoutputï¼‰
- `--no-headless`ï¼šæ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢
- `--json-output`ï¼šä»…è¾“å‡ºJSONæ ¼å¼
- `--max-retries`ï¼šæœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š3ï¼‰

### æ‰¹é‡çˆ¬å–

å¤„ç†å¤§é‡åœ°ç‚¹æ•°æ®ï¼š

```bash
# åŸºç¡€æ‰¹é‡å¤„ç†
python batch_scraper.py input/places.jsonl

# æŒ‡å®šå¹¶å‘æ•°å’Œå¤„ç†æ•°é‡
python batch_scraper.py input/places.jsonl --max-workers 5 --max-places 100

# å¯ç”¨æ–­ç‚¹æ¢å¤
python batch_scraper.py input/places.jsonl --resume

# é‡è¯•å¤±è´¥çš„åœ°ç‚¹
python batch_scraper.py input/places.jsonl --retry-failed

# æ— äººå€¼å®ˆæ¨¡å¼ï¼ˆé•¿æœŸè¿è¡Œï¼‰
python batch_scraper.py input/places.jsonl --unattended

# æ— é™è¿è¡Œæ¨¡å¼
python batch_scraper.py input/places.jsonl --infinite
```

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### æ£€æŸ¥å¤„ç†è¿›åº¦

```bash
python check_progress.py
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
=== Progress.json ç»Ÿè®¡ ===
successful count: 1250
failed count: 45
skipped count: 0
total processed: 1295
```

### é‡è¯•å¤±è´¥çš„çˆ¬å–

```bash
python retry_failed_scrapes.py
```

### æŸ¥çœ‹è¾“å…¥æ•°æ®ç»Ÿè®¡

```bash
python input_statistics.py input/places.jsonl
```

## âš™ï¸ é…ç½®é€‰é¡¹

### é¢„è®¾é…ç½®æ¨¡å¼

- **é»˜è®¤æ¨¡å¼**ï¼šå¹³è¡¡çš„æ€§èƒ½å’Œç¨³å®šæ€§
- **æµ‹è¯•æ¨¡å¼**ï¼šç”¨äºå¿«é€Ÿæµ‹è¯•ï¼ˆ`--test`ï¼‰
- **ç”Ÿäº§æ¨¡å¼**ï¼šé«˜æ€§èƒ½å¤§è§„æ¨¡å¤„ç†ï¼ˆ`--production`ï¼‰
- **æ— äººå€¼å®ˆæ¨¡å¼**ï¼šé•¿æœŸè‡ªåŠ¨è¿è¡Œï¼ˆ`--unattended`ï¼‰
- **æ— é™æ¨¡å¼**ï¼šæŒç»­è¿è¡Œç›´åˆ°æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼ˆ`--infinite`ï¼‰
- **å¿«é€Ÿæ¨¡å¼**ï¼šé«˜é€Ÿå¤„ç†ï¼Œè¾ƒå°‘é‡è¯•ï¼ˆ`--fast`ï¼‰

### ä¸»è¦å‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--max-workers` | æœ€å¤§å¹¶å‘çº¿ç¨‹æ•° | 3 |
| `--max-retries` | å•ä¸ªåœ°ç‚¹æœ€å¤§é‡è¯•æ¬¡æ•° | 3 |
| `--timeout` | å•ä¸ªåœ°ç‚¹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ | 300 |
| `--max-places` | å•æ¬¡æœ€å¤§å¤„ç†åœ°ç‚¹æ•° | æ— é™åˆ¶ |
| `--resume` | å¯ç”¨æ–­ç‚¹æ¢å¤ | True |
| `--retry-failed` | é‡è¯•å¤±è´¥çš„åœ°ç‚¹ | False |

## ğŸ“„ è¾“å‡ºæ ¼å¼

### JSONæ ¼å¼

æ¯ä¸ªå•†æˆ·çš„æ•°æ®ä¿å­˜ä¸ºç‹¬ç«‹çš„JSONæ–‡ä»¶ï¼š

```json
{
    "place_id": "ChIJ...",
    "business_info": {
        "name": "å•†æˆ·åç§°",
        "address": "è¯¦ç»†åœ°å€",
        "phone": "è”ç³»ç”µè¯",
        "website": "å®˜æ–¹ç½‘ç«™",
        "rating": 4.5,
        "total_reviews": 1250,
        "coordinates": {
            "latitude": 40.7128,
            "longitude": -74.0060
        },
        "hours": "è¥ä¸šæ—¶é—´ä¿¡æ¯"
    },
    "reviews": [
        {
            "author": "ç”¨æˆ·å",
            "rating": 5,
            "date": "2024-01-15",
            "text": "è¯„è®ºå†…å®¹",
            "helpful_count": 12
        }
    ],
    "scrape_info": {
        "scraped_at": "2024-01-15T10:30:00",
        "total_reviews_scraped": 45,
        "scrape_duration": 120.5
    }
}
```

### CSVæ ¼å¼

åŒæ—¶ç”ŸæˆCSVæ ¼å¼çš„è¯„è®ºæ•°æ®ï¼Œä¾¿äºè¿›ä¸€æ­¥åˆ†æã€‚

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Chromeæµè§ˆå™¨å¯åŠ¨å¤±è´¥**
   - ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨
   - æ£€æŸ¥ChromeDriverç‰ˆæœ¬å…¼å®¹æ€§

2. **ç½‘ç»œè¶…æ—¶**
   - å¢åŠ  `--timeout` å‚æ•°å€¼
   - æ£€æŸ¥ç½‘ç»œè¿æ¥ç¨³å®šæ€§

3. **éªŒè¯ç æˆ–åçˆ¬è™«æ£€æµ‹**
   - é™ä½å¹¶å‘æ•°ï¼ˆ`--max-workers 1`ï¼‰
   - å¢åŠ éšæœºå»¶è¿Ÿ
   - ä½¿ç”¨ä»£ç†IP

4. **å†…å­˜ä¸è¶³**
   - é™ä½ `--max-workers` å€¼
   - ä½¿ç”¨ `--max-places` åˆ†æ‰¹å¤„ç†

### é”™è¯¯ä»£ç 

è¯¦ç»†çš„é”™è¯¯ä»£ç è¯´æ˜è¯·æŸ¥çœ‹ [error_codes.md](error_codes.md)

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### æ¨èé…ç½®

- **å°è§„æ¨¡æµ‹è¯•**ï¼ˆ<100ä¸ªåœ°ç‚¹ï¼‰ï¼š`--max-workers 1 --max-places 10`
- **ä¸­ç­‰è§„æ¨¡**ï¼ˆ100-1000ä¸ªåœ°ç‚¹ï¼‰ï¼š`--max-workers 3 --max-places 100`
- **å¤§è§„æ¨¡å¤„ç†**ï¼ˆ>1000ä¸ªåœ°ç‚¹ï¼‰ï¼š`--max-workers 5 --max-places 500`

### æ³¨æ„äº‹é¡¹

- è¿‡é«˜çš„å¹¶å‘å¯èƒ½è§¦å‘åçˆ¬è™«æœºåˆ¶
- å»ºè®®åœ¨éé«˜å³°æ—¶æ®µè¿è¡Œå¤§è§„æ¨¡çˆ¬å–
- å®šæœŸæ£€æŸ¥å’Œæ¸…ç†è¾“å‡ºç›®å½•

## ğŸ”§ å¼€å‘è¯´æ˜

### æ‰©å±•åŠŸèƒ½

é¡¹ç›®é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå¯ä»¥è½»æ¾æ‰©å±•ï¼š

- æ·»åŠ æ–°çš„æ•°æ®æå–å­—æ®µ
- è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼
- é›†æˆä¸åŒçš„å­˜å‚¨åç«¯
- æ·»åŠ æ•°æ®æ¸…æ´—å’ŒéªŒè¯

### è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. åˆ›å»º Pull Request

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š

1. æŸ¥çœ‹é”™è¯¯ä»£ç æ–‡æ¡£
2. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
3. æäº¤Issueæè¿°é—®é¢˜

## âš–ï¸ å…è´£å£°æ˜

æœ¬å·¥å…·ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„ã€‚ä½¿ç”¨è€…éœ€è¦ï¼š

- éµå®ˆGoogle Mapsçš„æœåŠ¡æ¡æ¬¾
- éµå®ˆç›¸å…³æ³•å¾‹æ³•è§„
- åˆç†æ§åˆ¶çˆ¬å–é¢‘ç‡
- å°Šé‡ç½‘ç«™çš„robots.txt

è¯·è´Ÿè´£ä»»åœ°ä½¿ç”¨æ­¤å·¥å…·ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆè¿‡å¤§è´Ÿæ‹…ã€‚

---

[ä¸­æ–‡ç‰ˆ](#google-maps-è¯„è®ºçˆ¬è™«)

# Google Maps Review Scraper

A powerful tool for scraping business information and user reviews from Google Maps, designed for large-scale data collection.

## ğŸŒŸ Key Features

- **Large-Scale Batch Processing**: Supports batch scraping of tens of thousands of places.
- **Intelligent Resume**: Automatically resumes processing after an unexpected interruption.
- **Multi-threaded Concurrency**: Supports multi-threaded processing to improve efficiency.
- **Error Retry Mechanism**: Intelligently retries failed places to increase success rate.
- **Detailed Progress Tracking**: Real-time display of processing progress and statistics.
- **Flexible Output Formats**: Supports JSON and CSV output formats.
- **Comprehensive Data Extraction**: Includes basic business information, review content, ratings, etc.

## ğŸ“š Documentation

- ğŸ“– **[Technical Documentation](TECHNICAL.md)** - Detailed code function descriptions and implementation details.
- ğŸ“‹ **[Error Codes](error_codes.md)** - Complete error code definitions and solutions.

## ğŸ“ Project Structure

```
googlemaps-scraper-reviews/
â”œâ”€â”€ main.py                    # Core script for single-place scraping
â”œâ”€â”€ batch_scraper.py          # Batch processing script
â”œâ”€â”€ process_urls.py           # URL pre-processing tool to force English interface
â”œâ”€â”€ check_progress.py         # Progress checking tool
â”œâ”€â”€ retry_failed_scrapes.py   # Failed scrapes retry tool
â”œâ”€â”€ find_duplicates_simple.py # Duplicate removal tool
â”œâ”€â”€ input_statistics.py       # Input data statistics tool
â”œâ”€â”€ error_codes.md           # Error code descriptions
â”œâ”€â”€ input/                   # Input data directory
â”œâ”€â”€ batch_output/           # Batch output data directory
â”‚   â”œâ”€â”€ culture/           # Data for cultural places
â”‚   â”œâ”€â”€ education/         # Data for educational places
â”‚   â”œâ”€â”€ entertainment_and_recreation/ # Data for entertainment and recreation
â”‚   â”œâ”€â”€ facilities/        # Data for facilities
â”‚   â”œâ”€â”€ food_and_drink/    # Data for food and drink
â”‚   â”œâ”€â”€ natural_features/  # Data for natural features
â”‚   â”œâ”€â”€ places_of_worship/ # Data for places of worship
â”‚   â”œâ”€â”€ shopping/          # Data for shopping
â”‚   â”œâ”€â”€ sports/            # Data for sports
â”‚   â”œâ”€â”€ transportation/    # Data for transportation
â”‚   â”œâ”€â”€ progress.json      # Processing progress log
â”‚   â”œâ”€â”€ errors.jsonl       # Error log
â”‚   â””â”€â”€ success.jsonl      # Success log
â””â”€â”€ backup/                # Backup data directory
```

## ğŸš€ Quick Start

### Requirements

- Python 3.7+
- Google Chrome Browser
- ChromeDriver (managed automatically)

### Installation

```bash
pip install selenium pandas tqdm
```

### Prepare Input Data

Place JSONL format input files in the `input/` directory. Each line should contain information for one place:

```json
{
    "place_id": "ChIJ...",
    "place_name": "Business Name",
    "parent_type": "food_and_drink",
    "sub_type": "restaurant",
    "url": "https://maps.google.com/..."
}
```

### URL Pre-processing: Force English Interface

**IMPORTANT:** The current version of the scraper can only recognize and parse the structure of **English Google Maps pages**. To ensure data is scraped correctly, all URLs must be pre-processed to force the page to display in English. Unprocessed URLs will lead to scraping failures.

The `process_urls.py` script in the project can be used to add the `&hl=en` parameter to URLs in bulk.

**Steps:**

1.  **Configure the script**: Open the `process_urls.py` file and modify the `input_file` and `output_file` variables according to your file locations.
2.  **Run the script**:
    ```bash
    python process_urls.py
    ```
3.  **Use the new file**: The script will generate a new processed file. Be sure to use this new file (e.g., `..._en.jsonl`) as the input for subsequent scraping operations.

### Single Place Scraping

Scrape data for a single place:

```bash
python main.py "https://maps.google.com/..."
```

Supported arguments:
- `--output-dir`: Output directory (default: output)
- `--no-headless`: Show browser UI
- `--json-output`: Output only in JSON format
- `--max-retries`: Maximum number of retries (default: 3)

### Batch Scraping

Process a large number of places:

```bash
# Basic batch processing
python batch_scraper.py input/places.jsonl

# Specify number of workers and places
python batch_scraper.py input/places.jsonl --max-workers 5 --max-places 100

# Enable resume functionality
python batch_scraper.py input/places.jsonl --resume

# Retry failed places
python batch_scraper.py input/places.jsonl --retry-failed

# Unattended mode (for long-running tasks)
python batch_scraper.py input/places.jsonl --unattended

# Infinite mode
python batch_scraper.py input/places.jsonl --infinite
```

## ğŸ“Š Monitoring and Management

### Check Progress

```bash
python check_progress.py
```

Example output:
```
=== Progress.json Statistics ===
successful count: 1250
failed count: 45
skipped count: 0
total processed: 1295
```

### Retry Failed Scrapes

```bash
python retry_failed_scrapes.py
```

### View Input Data Statistics

```bash
python input_statistics.py input/places.jsonl
```

## âš™ï¸ Configuration Options

### Preset Configuration Modes

- **Default Mode**: Balanced performance and stability.
- **Test Mode**: For quick testing (`--test`).
- **Production Mode**: High-performance for large-scale processing (`--production`).
- **Unattended Mode**: For long-term automatic operation (`--unattended`).
- **Infinite Mode**: Runs continuously until all data is processed (`--infinite`).
- **Fast Mode**: High-speed processing with fewer retries (`--fast`).

### Main Parameters

| Parameter | Description | Default |
|---|---|---|
| `--max-workers` | Maximum number of concurrent workers | 3 |
| `--max-retries` | Max retries for a single place | 3 |
| `--timeout` | Timeout for a single place (seconds) | 300 |
| `--max-places` | Maximum number of places to process | unlimited |
| `--resume` | Enable resume functionality | True |
| `--retry-failed` | Retry failed places | False |

## ğŸ“„ Output Format

### JSON Format

Data for each business is saved in a separate JSON file:

```json
{
    "place_id": "ChIJ...",
    "business_info": {
        "name": "Business Name",
        "address": "Detailed Address",
        "phone": "Contact Phone",
        "website": "Official Website",
        "rating": 4.5,
        "total_reviews": 1250,
        "coordinates": {
            "latitude": 40.7128,
            "longitude": -74.0060
        },
        "hours": "Business Hours Information"
    },
    "reviews": [
        {
            "author": "Username",
            "rating": 5,
            "date": "2024-01-15",
            "text": "Review content",
            "helpful_count": 12
        }
    ],
    "scrape_info": {
        "scraped_at": "2024-01-15T10:30:00",
        "total_reviews_scraped": 45,
        "scrape_duration": 120.5
    }
}
```

### CSV Format

CSV format review data is also generated for further analysis.

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **Chrome Browser Fails to Start**
   - Ensure Google Chrome is installed.
   - Check ChromeDriver version compatibility.

2. **Network Timeout**
   - Increase the `--timeout` parameter value.
   - Check network connection stability.

3. **CAPTCHA or Anti-scraping Detection**
   - Reduce concurrency (`--max-workers 1`).
   - Increase random delays.
   - Use proxy IPs.

4. **Insufficient Memory**
   - Decrease the `--max-workers` value.
   - Use `--max-places` to process in batches.

### Error Codes

For detailed error code descriptions, please see [error_codes.md](error_codes.md).

## ğŸ“ˆ Performance Optimization

### Recommended Configurations

- **Small-scale testing** (<100 places): `--max-workers 1 --max-places 10`
- **Medium-scale** (100-1000 places): `--max-workers 3 --max-places 100`
- **Large-scale processing** (>1000 places): `--max-workers 5 --max-places 500`

### Notes

- High concurrency may trigger anti-scraping mechanisms.
- It's recommended to run large-scale scraping during off-peak hours.
- Periodically check and clean the output directory.

## ğŸ”§ Development

### Extending Functionality

The project uses a modular design, making it easy to extend:

- Add new data extraction fields.
- Customize output formats.
- Integrate different storage backends.
- Add data cleaning and validation.

### Contribution Guidelines

1. Fork the project.
2. Create a feature branch.
3. Commit your changes.
4. Create a Pull Request.

## ğŸ“ Support

If you have questions or suggestions, please:

1. Check the error code documentation.
2. Check the log files.
3. Submit an Issue describing the problem.

## âš–ï¸ Disclaimer

This tool is for learning and research purposes only. Users must:

- Comply with Google Maps' Terms of Service.
- Adhere to relevant laws and regulations.
- Control scraping frequency responsibly.
- Respect the website's robots.txt file.

Please use this tool responsibly and avoid placing an excessive burden on the target website.