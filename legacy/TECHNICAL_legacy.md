# æŠ€æœ¯æ–‡æ¡£ - ä»£ç è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»é¡¹ç›®ä¸­å„ä¸ªPythonè„šæœ¬çš„åŠŸèƒ½ã€ç”¨æ³•å’Œå®ç°ç»†èŠ‚ã€‚

## ğŸ“ ç›®å½•

- [æ ¸å¿ƒçˆ¬è™«æ¨¡å—](#æ ¸å¿ƒçˆ¬è™«æ¨¡å—)
  - [main.py - å•ç‚¹çˆ¬è™«](#mainpy---å•ç‚¹çˆ¬è™«)
  - [batch_scraper.py - æ‰¹é‡å¤„ç†å™¨](#batch_scraperpy---æ‰¹é‡å¤„ç†å™¨)
- [å·¥å…·æ¨¡å—](#å·¥å…·æ¨¡å—)
  - [process_urls.py - URLé¢„å¤„ç†å™¨](#process_urlspy---urlé¢„å¤„ç†å™¨)
  - [check_progress.py - è¿›åº¦æ£€æŸ¥å™¨](#check_progresspy---è¿›åº¦æ£€æŸ¥å™¨)
  - [retry_failed_scrapes.py - å¤±è´¥é‡è¯•å™¨](#retry_failed_scrapespy---å¤±è´¥é‡è¯•å™¨)
  - [find_duplicates_simple.py - é‡å¤æ•°æ®æ£€æµ‹](#find_duplicates_simplepy---é‡å¤æ•°æ®æ£€æµ‹)
  - [input_statistics.py - è¾“å…¥æ•°æ®ç»Ÿè®¡](#input_statisticspy---è¾“å…¥æ•°æ®ç»Ÿè®¡)
- [é…ç½®å’Œé”™è¯¯å¤„ç†](#é…ç½®å’Œé”™è¯¯å¤„ç†)
  - [error_codes.md - é”™è¯¯ä»£ç è¯´æ˜](#error_codesmd---é”™è¯¯ä»£ç è¯´æ˜)

---

## æ ¸å¿ƒçˆ¬è™«æ¨¡å—

### main.py - å•ç‚¹çˆ¬è™«

**åŠŸèƒ½æè¿°**ï¼šGoogle Mapså•ä¸ªåœ°ç‚¹çš„æ•°æ®çˆ¬å–æ ¸å¿ƒå¼•æ“

#### ä¸»è¦ç‰¹æ€§

- **å®Œæ•´æ•°æ®æå–**ï¼šå•†æˆ·åŸºæœ¬ä¿¡æ¯ã€è¯„è®ºå†…å®¹ã€è¯„åˆ†ã€åæ ‡ç­‰
- **æ™ºèƒ½é‡è¯•æœºåˆ¶**ï¼šç½‘ç»œè¶…æ—¶å’Œä¸´æ—¶é”™è¯¯è‡ªåŠ¨é‡è¯•
- **å¤šç§è¾“å‡ºæ ¼å¼**ï¼šJSONå’ŒCSVæ ¼å¼åŒæ—¶è¾“å‡º
- **é”™è¯¯åˆ†ç±»å¤„ç†**ï¼šè¯¦ç»†çš„é”™è¯¯ä»£ç å’Œé”™è¯¯ä¿¡æ¯
- **ç¼–ç å…¼å®¹æ€§**ï¼šå®Œç¾æ”¯æŒä¸­æ–‡å­—ç¬¦å’Œç‰¹æ®Šå­—ç¬¦

#### æ ¸å¿ƒåŠŸèƒ½

1. **å•†æˆ·ä¿¡æ¯æå–**
   ```python
   business_info = {
       "name": "å•†æˆ·åç§°",
       "address": "è¯¦ç»†åœ°å€", 
       "phone": "è”ç³»ç”µè¯",
       "website": "å®˜æ–¹ç½‘ç«™",
       "rating": 4.5,
       "total_reviews": 1250,
       "coordinates": {"latitude": 40.7128, "longitude": -74.0060},
       "hours": "è¥ä¸šæ—¶é—´ä¿¡æ¯"
   }
   ```

2. **è¯„è®ºæ•°æ®æå–**
   ```python
   review = {
       "author": "ç”¨æˆ·å",
       "rating": 5,
       "date": "2024-01-15", 
       "text": "è¯„è®ºå†…å®¹",
       "helpful_count": 12
   }
   ```

#### ä½¿ç”¨æ–¹æ³•

```bash
# åŸºç¡€ç”¨æ³•
python main.py "https://maps.google.com/maps/place/..."

# é«˜çº§å‚æ•°
python main.py "URL" \
    --output-dir custom_output \
    --max-retries 5 \
    --no-headless \
    --json-output
```

#### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `url` | string | å¿…éœ€ | Google Mapsåœ°ç‚¹URL |
| `--output-dir` | string | "output" | è¾“å‡ºç›®å½•è·¯å¾„ |
| `--max-retries` | int | 3 | æœ€å¤§é‡è¯•æ¬¡æ•° |
| `--no-headless` | flag | False | æ˜¾ç¤ºæµè§ˆå™¨ç•Œé¢ |
| `--json-output` | flag | False | ä»…è¾“å‡ºJSONæ ¼å¼ |

#### é”™è¯¯å¤„ç†

è„šæœ¬ä½¿ç”¨æ ‡å‡†åŒ–é”™è¯¯ä»£ç ç³»ç»Ÿï¼š

- `1001`: æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥
- `1002`: URLåŠ è½½å¤±è´¥
- `1003`: å•†æˆ·ä¿¡æ¯æå–å¤±è´¥
- `1004`: è¯„è®ºæŒ‰é’®æœªæ‰¾åˆ°
- `1005`: è¯„è®ºæ»šåŠ¨å¤±è´¥
- `1006`: è¯„è®ºæå–å¤±è´¥
- `1007`: åæ ‡æå–å¤±è´¥
- `1008`: CSVä¿å­˜å¤±è´¥

---

### batch_scraper.py - æ‰¹é‡å¤„ç†å™¨

**åŠŸèƒ½æè¿°**ï¼šå¤§è§„æ¨¡Google Mapsæ•°æ®æ‰¹é‡çˆ¬å–å·¥å…·ï¼Œæ”¯æŒæ•°ä¸‡çº§åœ°ç‚¹å¤„ç†

#### ä¸»è¦ç‰¹æ€§

- **å¤§è§„æ¨¡å¤„ç†**ï¼šæ”¯æŒæ•°ä¸‡ä¸ªåœ°ç‚¹çš„æ‰¹é‡å¤„ç†
- **æ–­ç‚¹æ¢å¤**ï¼šæ„å¤–ä¸­æ–­åè‡ªåŠ¨ä»æ–­ç‚¹ç»§ç»­
- **å¤šçº¿ç¨‹å¹¶å‘**ï¼šå¯é…ç½®çš„å¹¶å‘çº¿ç¨‹æ•°
- **æ™ºèƒ½é‡è¯•**ï¼šå¤±è´¥åœ°ç‚¹çš„é€‰æ‹©æ€§é‡è¯•
- **è¿›åº¦è·Ÿè¸ª**ï¼šå®æ—¶è¿›åº¦æ˜¾ç¤ºå’ŒçŠ¶æ€ä¿å­˜
- **å¤šç§è¿è¡Œæ¨¡å¼**ï¼šæµ‹è¯•ã€ç”Ÿäº§ã€æ— äººå€¼å®ˆç­‰æ¨¡å¼

#### æ ¸å¿ƒæ¶æ„

```python
class BatchGoogleMapsScraper:
    def __init__(self, script_path, output_base_dir, config):
        # åˆå§‹åŒ–é…ç½®å’Œè·¯å¾„
        
    def process_batch(self, places, **kwargs):
        # æ‰¹é‡å¤„ç†ä¸»é€»è¾‘
        
    def scrape_single_place(self, place_data):
        # å•ä¸ªåœ°ç‚¹å¤„ç†
        
    def save_progress(self, progress):
        # ä¿å­˜å¤„ç†è¿›åº¦
```

#### é…ç½®æ¨¡å¼

1. **é»˜è®¤æ¨¡å¼** - å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
   ```python
   DEFAULT_CONFIG = {
       'max_workers': 3,
       'max_retries': 3,
       'timeout': 300,
       'max_places': None
   }
   ```

2. **æµ‹è¯•æ¨¡å¼** - å¿«é€ŸéªŒè¯åŠŸèƒ½
   ```python
   TEST_CONFIG = {
       'max_workers': 1,
       'max_places': 5,
       'timeout': 60
   }
   ```

3. **ç”Ÿäº§æ¨¡å¼** - å¤§è§„æ¨¡å¤„ç†
   ```python
   PRODUCTION_CONFIG = {
       'max_workers': 5,
       'max_places': 1000,
       'timeout': 600
   }
   ```

4. **æ— äººå€¼å®ˆæ¨¡å¼** - é•¿æœŸè‡ªåŠ¨è¿è¡Œ
   ```python
   UNATTENDED_CONFIG = {
       'max_workers': 6,
       'max_places': 120,
       'continuous_mode': True,
       'auto_retry_failed': True
   }
   ```

#### ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºç¡€æ‰¹é‡å¤„ç†
python batch_scraper.py input/places.jsonl

# æµ‹è¯•æ¨¡å¼
python batch_scraper.py input/places.jsonl --test

# ç”Ÿäº§æ¨¡å¼
python batch_scraper.py input/places.jsonl --production \
    --max-workers 8 --max-places 2000

# æ— äººå€¼å®ˆæ¨¡å¼
python batch_scraper.py input/places.jsonl --unattended

# è‡ªå®šä¹‰é…ç½®
python batch_scraper.py input/places.jsonl \
    --max-workers 4 \
    --max-retries 5 \
    --timeout 600 \
    --max-places 500 \
    --resume \
    --retry-failed
```

#### è¾“å‡ºæ–‡ä»¶ç»“æ„

```
batch_output/
â”œâ”€â”€ parent_type/
â”‚   â””â”€â”€ sub_type/
â”‚       â”œâ”€â”€ place_id_å•†æˆ·åç§°.json
â”‚       â””â”€â”€ place_id_å•†æˆ·åç§°.csv
â”œâ”€â”€ progress.json          # å¤„ç†è¿›åº¦
â”œâ”€â”€ errors.jsonl          # é”™è¯¯æ—¥å¿—
â””â”€â”€ success.jsonl         # æˆåŠŸæ—¥å¿—
```

---

## å·¥å…·æ¨¡å—

### process_urls.py - URLé¢„å¤„ç†å™¨

**åŠŸèƒ½æè¿°**ï¼šæ‰¹é‡ä¸ºè¾“å…¥æ–‡ä»¶ä¸­çš„Google Maps URLæ·»åŠ `&hl=en`å‚æ•°ï¼Œä»¥å¼ºåˆ¶æµè§ˆå™¨ä½¿ç”¨è‹±æ–‡ç•Œé¢ã€‚è¿™æ˜¯ç¡®ä¿çˆ¬è™«èƒ½æ­£ç¡®è§£æé¡µé¢çš„å…³é”®é¢„å¤„ç†æ­¥éª¤ã€‚

#### ä¸»è¦ç‰¹æ€§
- **å¼ºåˆ¶è‹±æ–‡**ï¼šç¡®ä¿æ‰€æœ‰URLéƒ½æŒ‡å‘è‹±æ–‡ç‰ˆGoogle Mapsé¡µé¢ã€‚
- **å¹‚ç­‰æ€§**ï¼šä¸ä¼šé‡å¤ä¸ºå·²ç»åŒ…å«`&hl=en`çš„URLæ·»åŠ å‚æ•°ã€‚
- **å®¹é”™æ€§**ï¼šä¼šè·³è¿‡æ–‡ä»¶ä¸­æ ¼å¼é”™è¯¯çš„JSONè¡Œã€‚

#### ä½¿ç”¨æ–¹æ³•
1.  **é…ç½®è·¯å¾„**ï¼šåœ¨ `process_urls.py` è„šæœ¬ä¸­ï¼Œä¿®æ”¹ `input_file` å’Œ `output_file` å˜é‡ä»¥åŒ¹é…ä½ çš„æ–‡ä»¶è·¯å¾„ã€‚
2.  **æ‰§è¡Œè„šæœ¬**ï¼š
    ```bash
    python process_urls.py
    ```
3.  **ä½¿ç”¨è¾“å‡º**ï¼šä½¿ç”¨è„šæœ¬ç”Ÿæˆçš„æ–°æ–‡ä»¶ï¼ˆä¾‹å¦‚ `..._en.jsonl`ï¼‰ä½œä¸º `batch_scraper.py` çš„è¾“å…¥ã€‚

#### æ ¸å¿ƒé€»è¾‘
```python
def process_urls():
    with open(input_file, 'r', encoding='utf-8') as infile, \
         open(output_file, 'w', encoding='utf-8') as outfile:
        for line in infile:
            try:
                data = json.loads(line)
                # In the actual script, the key is 'Maps_url'. 
                # This is a representative example.
                if 'url' in data and data['url']:
                    if '&hl=en' not in data['url']:
                        data['url'] += '&hl=en'
                outfile.write(json.dumps(data, ensure_ascii=False) + '\n')
            except json.JSONDecodeError:
                print(f"Skipping malformed JSON line: {line.strip()}")
```

### check_progress.py - è¿›åº¦æ£€æŸ¥å™¨

**åŠŸèƒ½æè¿°**ï¼šå¿«é€Ÿæ£€æŸ¥æ‰¹é‡å¤„ç†çš„è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯

#### ä¸»è¦åŠŸèƒ½

- **è¿›åº¦ç»Ÿè®¡**ï¼šæˆåŠŸã€å¤±è´¥ã€è·³è¿‡çš„åœ°ç‚¹æ•°é‡
- **é‡å¤æ£€æµ‹**ï¼šæ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å¤å¤„ç†çš„åœ°ç‚¹
- **çŠ¶æ€åˆ†æ**ï¼šåˆ†æå¤„ç†çŠ¶æ€çš„åˆ†å¸ƒæƒ…å†µ

#### ä½¿ç”¨æ–¹æ³•

```bash
python check_progress.py
```

#### è¾“å‡ºç¤ºä¾‹

```
=== Progress.json ç»Ÿè®¡ ===
successful count: 1250
failed count: 45
skipped count: 0
total processed: 1295

=== å»é‡åç»Ÿè®¡ ===
unique successful: 1250
unique failed: 45
unique skipped: 0

=== äº¤é›†æ£€æŸ¥ ===
successful & failed: 0
successful & skipped: 0
failed & skipped: 0

=== æ€»è®¡ ===
total unique processed IDs: 1295
```

#### ä»£ç è§£æ

```python
# è¯»å–è¿›åº¦æ–‡ä»¶
with open('batch_output/progress.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# ç»Ÿè®¡å„çŠ¶æ€æ•°é‡
successful_count = len(data['successful'])
failed_count = len(data['failed'])
skipped_count = len(data['skipped'])

# æ£€æŸ¥é‡å¤å’Œäº¤é›†
successful_set = set(data['successful'])
failed_set = set(data['failed'])
intersection = successful_set & failed_set
```

---

### retry_failed_scrapes.py - å¤±è´¥é‡è¯•å™¨

**åŠŸèƒ½æè¿°**ï¼šä¸“é—¨é‡è¯•ä¹‹å‰å¤±è´¥çš„åœ°ç‚¹ï¼Œæé«˜æ•°æ®å®Œæ•´æ€§

#### ä¸»è¦ç‰¹æ€§

- **æ™ºèƒ½ç­›é€‰**ï¼šè‡ªåŠ¨è¯†åˆ«å¤±è´¥çš„åœ°ç‚¹
- **æ‰¹é‡é‡è¯•**ï¼šæ”¯æŒæ‰¹é‡é‡æ–°å¤„ç†å¤±è´¥é¡¹ç›®
- **é…ç½®ç»§æ‰¿**ï¼šç»§æ‰¿åŸæœ‰çš„æ‰¹é‡å¤„ç†é…ç½®
- **è¿›åº¦åˆå¹¶**ï¼šé‡è¯•ç»“æœè‡ªåŠ¨åˆå¹¶åˆ°ä¸»è¿›åº¦æ–‡ä»¶

#### ä½¿ç”¨æ–¹æ³•

```bash
# åŸºç¡€é‡è¯•
python retry_failed_scrapes.py

# æŒ‡å®šè¾“å…¥æ–‡ä»¶å’Œé…ç½®
python retry_failed_scrapes.py \
    --input input/places.jsonl \
    --max-workers 3 \
    --max-retries 5
```

#### å·¥ä½œæµç¨‹

1. **è¯»å–è¿›åº¦æ–‡ä»¶**ï¼šä»`progress.json`è·å–å¤±è´¥åˆ—è¡¨
2. **åŠ è½½åŸå§‹æ•°æ®**ï¼šä»è¾“å…¥æ–‡ä»¶ä¸­æ‰¾åˆ°å¯¹åº”çš„åœ°ç‚¹ä¿¡æ¯
3. **æ‰§è¡Œé‡è¯•**ï¼šä½¿ç”¨batch_scraperé‡æ–°å¤„ç†
4. **æ›´æ–°è¿›åº¦**ï¼šå°†ç»“æœæ›´æ–°åˆ°è¿›åº¦æ–‡ä»¶

#### æ ¸å¿ƒé€»è¾‘

```python
def retry_failed_places():
    # 1. è¯»å–å¤±è´¥åˆ—è¡¨
    progress = load_progress()
    failed_place_ids = progress.get('failed', [])
    
    # 2. åŠ è½½åŸå§‹åœ°ç‚¹æ•°æ®
    original_places = load_input_data()
    failed_places = filter_failed_places(original_places, failed_place_ids)
    
    # 3. é‡æ–°å¤„ç†
    scraper = BatchGoogleMapsScraper()
    scraper.process_batch(failed_places, retry_mode=True)
    
    # 4. æ›´æ–°è¿›åº¦
    update_progress(results)
```

---

### find_duplicates_simple.py - é‡å¤æ•°æ®æ£€æµ‹

**åŠŸèƒ½æè¿°**ï¼šæ£€æµ‹å’Œå¤„ç†è¾“å‡ºæ•°æ®ä¸­çš„é‡å¤é¡¹ç›®

#### ä¸»è¦åŠŸèƒ½

- **é‡å¤æ£€æµ‹**ï¼šåŸºäºplace_idæ£€æµ‹é‡å¤çš„æ•°æ®æ–‡ä»¶
- **ç»Ÿè®¡æŠ¥å‘Š**ï¼šç”Ÿæˆè¯¦ç»†çš„é‡å¤æƒ…å†µæŠ¥å‘Š
- **æ¸…ç†å»ºè®®**ï¼šæä¾›é‡å¤æ–‡ä»¶çš„æ¸…ç†å»ºè®®

#### ä½¿ç”¨æ–¹æ³•

```bash
python find_duplicates_simple.py
```

#### æ£€æµ‹é€»è¾‘

```python
def find_duplicates():
    file_dict = {}
    duplicates = []
    
    # éå†æ‰€æœ‰è¾“å‡ºæ–‡ä»¶
    for file_path in glob("batch_output/**/*.json"):
        place_id = extract_place_id(file_path)
        
        if place_id in file_dict:
            duplicates.append({
                'place_id': place_id,
                'files': [file_dict[place_id], file_path]
            })
        else:
            file_dict[place_id] = file_path
    
    return duplicates
```

#### è¾“å‡ºæŠ¥å‘Š

```
=== é‡å¤æ–‡ä»¶æ£€æµ‹æŠ¥å‘Š ===
æ€»æ–‡ä»¶æ•°: 1295
å”¯ä¸€place_idæ•°: 1290
é‡å¤place_idæ•°: 5

é‡å¤è¯¦æƒ…:
place_id: ChIJ123...
  - batch_output/food_and_drink/restaurant/ChIJ123_é¤å…A.json
  - batch_output/food_and_drink/chinese_restaurant/ChIJ123_é¤å…A.json
```

---

### input_statistics.py - è¾“å…¥æ•°æ®ç»Ÿè®¡

**åŠŸèƒ½æè¿°**ï¼šåˆ†æè¾“å…¥æ•°æ®çš„åˆ†å¸ƒå’Œç»Ÿè®¡ä¿¡æ¯

#### ä¸»è¦åŠŸèƒ½

- **ç±»å‹åˆ†å¸ƒ**ï¼šç»Ÿè®¡å„parent_typeå’Œsub_typeçš„æ•°é‡
- **æ•°æ®è´¨é‡**ï¼šæ£€æŸ¥ç¼ºå¤±å­—æ®µå’Œæ ¼å¼é—®é¢˜
- **åœ°ç†åˆ†å¸ƒ**ï¼šåˆ†æåœ°ç‚¹çš„åœ°ç†åˆ†å¸ƒæƒ…å†µï¼ˆå¦‚æœæœ‰åæ ‡ï¼‰

#### ä½¿ç”¨æ–¹æ³•

```bash
python input_statistics.py input/places.jsonl
```

#### åˆ†æç»´åº¦

1. **æ•°æ®æ€»é‡ç»Ÿè®¡**
   ```python
   total_places = len(places)
   unique_place_ids = len(set(p['place_id'] for p in places))
   ```

2. **ç±»å‹åˆ†å¸ƒç»Ÿè®¡**
   ```python
   parent_type_dist = Counter(p['parent_type'] for p in places)
   sub_type_dist = Counter(p['sub_type'] for p in places)
   ```

3. **æ•°æ®è´¨é‡æ£€æŸ¥**
   ```python
   missing_fields = {
       'place_id': sum(1 for p in places if not p.get('place_id')),
       'url': sum(1 for p in places if not p.get('url')),
       'place_name': sum(1 for p in places if not p.get('place_name'))
   }
   ```

#### è¾“å‡ºç¤ºä¾‹

```
=== è¾“å…¥æ•°æ®ç»Ÿè®¡æŠ¥å‘Š ===

åŸºç¡€ç»Ÿè®¡:
æ€»åœ°ç‚¹æ•°: 15,847
å”¯ä¸€place_idæ•°: 15,847
é‡å¤place_idæ•°: 0

Parent Typeåˆ†å¸ƒ:
food_and_drink: 8,234 (52.0%)
shopping: 3,456 (21.8%)
entertainment_and_recreation: 2,123 (13.4%)
transportation: 1,234 (7.8%)
å…¶ä»–: 800 (5.0%)

Sub Typeåˆ†å¸ƒ (Top 10):
restaurant: 3,456
store: 2,345
cafe: 1,234
park: 987
...

æ•°æ®è´¨é‡æ£€æŸ¥:
ç¼ºå¤±place_id: 0
ç¼ºå¤±URL: 12
ç¼ºå¤±place_name: 5
æ— æ•ˆURLæ ¼å¼: 3
```

---

## é…ç½®å’Œé”™è¯¯å¤„ç†

### error_codes.md - é”™è¯¯ä»£ç è¯´æ˜

**åŠŸèƒ½æè¿°**ï¼šæ ‡å‡†åŒ–é”™è¯¯ä»£ç å®šä¹‰å’Œé—®é¢˜è§£å†³æ–¹æ¡ˆ

#### é”™è¯¯ä»£ç åˆ†ç±»

1. **æµè§ˆå™¨ç›¸å…³é”™è¯¯ (1000-1099)**
   - `1001`: æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥
   - `1002`: URLåŠ è½½å¤±è´¥

2. **æ•°æ®æå–é”™è¯¯ (1100-1199)**
   - `1003`: å•†æˆ·ä¿¡æ¯æå–å¤±è´¥
   - `1004`: è¯„è®ºæŒ‰é’®æœªæ‰¾åˆ°
   - `1005`: è¯„è®ºæ»šåŠ¨å¤±è´¥
   - `1006`: è¯„è®ºæå–å¤±è´¥
   - `1007`: åæ ‡æå–å¤±è´¥

3. **æ–‡ä»¶æ“ä½œé”™è¯¯ (1200-1299)**
   - `1008`: CSVä¿å­˜å¤±è´¥

4. **ç½‘ç»œç›¸å…³é”™è¯¯ (1300-1399)**
   - `1009`: ç½‘ç»œè¶…æ—¶

5. **é€šç”¨é”™è¯¯ (1900-1999)**
   - `1010`: å…ƒç´ æœªæ‰¾åˆ°
   - `1999`: æœªé¢„æœŸé”™è¯¯

#### é”™è¯¯å¤„ç†ç­–ç•¥

```python
class ErrorCodes:
    SUCCESS = 0
    BROWSER_INIT_FAILED = 1001
    URL_LOAD_FAILED = 1002
    # ... å…¶ä»–é”™è¯¯ä»£ç 
    
def handle_error(error_code, error_message):
    """é”™è¯¯å¤„ç†å’Œé‡è¯•ç­–ç•¥"""
    if error_code in [1001, 1002]:
        # æµè§ˆå™¨ç›¸å…³é”™è¯¯ï¼Œé‡å¯æµè§ˆå™¨
        return "restart_browser"
    elif error_code in [1009, 1010]:
        # ç½‘ç»œæˆ–å…ƒç´ é—®é¢˜ï¼Œå¯é‡è¯•
        return "retry"
    else:
        # å…¶ä»–é”™è¯¯ï¼Œè®°å½•å¹¶è·³è¿‡
        return "skip"
```

---

## ğŸ”§ å¼€å‘å’Œæ‰©å±•

### æ·»åŠ æ–°åŠŸèƒ½

1. **æ‰©å±•æ•°æ®æå–å­—æ®µ**
   ```python
   # åœ¨main.pyä¸­æ·»åŠ æ–°çš„æå–é€»è¾‘
   def extract_additional_info(driver):
       # æ–°çš„æ•°æ®æå–é€»è¾‘
       pass
   ```

2. **è‡ªå®šä¹‰è¾“å‡ºæ ¼å¼**
   ```python
   # åœ¨batch_scraper.pyä¸­æ·»åŠ æ–°çš„è¾“å‡ºå¤„ç†å™¨
   def save_custom_format(data, output_path):
       # è‡ªå®šä¹‰æ ¼å¼ä¿å­˜é€»è¾‘
       pass
   ```

3. **é›†æˆæ•°æ®åº“å­˜å‚¨**
   ```python
   # æ·»åŠ æ•°æ®åº“è¿æ¥å’Œå­˜å‚¨é€»è¾‘
   def save_to_database(data):
       # æ•°æ®åº“ä¿å­˜é€»è¾‘
       pass
   ```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. **å†…å­˜ä¼˜åŒ–**ï¼šä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ–‡ä»¶
2. **å¹¶å‘ä¼˜åŒ–**ï¼šæ ¹æ®æœºå™¨æ€§èƒ½è°ƒæ•´çº¿ç¨‹æ•°
3. **ç½‘ç»œä¼˜åŒ–**ï¼šæ·»åŠ è¯·æ±‚é—´éš”å’Œé‡è¯•æœºåˆ¶
4. **å­˜å‚¨ä¼˜åŒ–**ï¼šå‹ç¼©è¾“å‡ºæ–‡ä»¶æˆ–ä½¿ç”¨æ›´é«˜æ•ˆçš„æ ¼å¼

### ç›‘æ§å’Œç»´æŠ¤

1. **æ—¥å¿—ç›‘æ§**ï¼šå®šæœŸæ£€æŸ¥é”™è¯¯æ—¥å¿—
2. **æ€§èƒ½ç›‘æ§**ï¼šè·Ÿè¸ªå¤„ç†é€Ÿåº¦å’ŒæˆåŠŸç‡
3. **æ•°æ®è´¨é‡**ï¼šå®šæœŸéªŒè¯è¾“å‡ºæ•°æ®çš„å®Œæ•´æ€§
4. **ç‰ˆæœ¬ç®¡ç†**ï¼šä¿æŒä»£ç å’Œä¾èµ–çš„æ›´æ–°

---

# Technical Documentation - Code Details

This document provides a detailed introduction to the functionality, usage, and implementation details of each Python script in the project.

## ğŸ“ Table of Contents

- [Core Scraper Modules](#core-scraper-modules)
  - [main.py - Single-Place Scraper](#mainpy---single-place-scraper)
  - [batch_scraper.py - Batch Processor](#batch_scraperpy---batch-processor)
- [Utility Modules](#utility-modules)
  - [process_urls.py - URL Pre-processor](#process_urlspy---url-pre-processor)
  - [check_progress.py - Progress Checker](#check_progresspy---progress-checker)
  - [retry_failed_scrapes.py - Failure Retrier](#retry_failed_scrapespy---failure-retrier)
  - [find_duplicates_simple.py - Duplicate Detection](#find_duplicates_simplepy---duplicate-detection)
  - [input_statistics.py - Input Data Statistics](#input_statisticspy---input-data-statistics)
- [Configuration and Error Handling](#configuration-and-error-handling)
  - [error_codes.md - Error Code Explanations](#error_codesmd---error-code-explanations)

---

## Core Scraper Modules

### main.py - Single-Place Scraper

**Functional Description**: The core engine for scraping data for a single Google Maps place.

#### Key Features

- **Comprehensive Data Extraction**: Business info, review content, ratings, coordinates, etc.
- **Intelligent Retry Mechanism**: Automatic retries for network timeouts and temporary errors.
- **Multiple Output Formats**: Simultaneous output in JSON and CSV formats.
- **Categorized Error Handling**: Detailed error codes and messages.
- **Encoding Compatibility**: Perfect support for Chinese and special characters.

#### Core Functions

1. **Business Information Extraction**
   ```python
   business_info = {
       "name": "Business Name",
       "address": "Detailed Address",
       "phone": "Contact Phone",
       "website": "Official Website",
       "rating": 4.5,
       "total_reviews": 1250,
       "coordinates": {"latitude": 40.7128, "longitude": -74.0060},
       "hours": "Business Hours Information"
   }
   ```

2. **Review Data Extraction**
   ```python
   review = {
       "author": "Username",
       "rating": 5,
       "date": "2024-01-15",
       "text": "Review content",
       "helpful_count": 12
   }
   ```

#### Usage

```bash
# Basic usage
python main.py "https://maps.google.com/maps/place/..."

# Advanced parameters
python main.py "URL" \
    --output-dir custom_output \
    --max-retries 5 \
    --no-headless \
    --json-output
```

#### Parameter Description

| Parameter | Type | Default | Description |
|---|---|---|---|
| `url` | string | Required | Google Maps place URL |
| `--output-dir` | string | "output" | Output directory path |
| `--max-retries` | int | 3 | Maximum number of retries |
| `--no-headless` | flag | False | Show browser UI |
| `--json-output` | flag | False | Output only in JSON format |

#### Error Handling

The script uses a standardized error code system:

- `1001`: Browser initialization failed
- `1002`: URL loading failed
- `1003`: Business info extraction failed
- `1004`: Reviews button not found
- `1005`: Review scrolling failed
- `1006`: Review extraction failed
- `1007`: Coordinate extraction failed
- `1008`: CSV save failed

---

### batch_scraper.py - Batch Processor

**Functional Description**: A tool for large-scale batch scraping of Google Maps data, supporting tens of thousands of places.

#### Key Features

- **Large-Scale Processing**: Supports batch processing of tens of thousands of places.
- **Resume from Breakpoint**: Automatically resumes from the last processed point after an interruption.
- **Multi-threaded Concurrency**: Configurable number of concurrent threads.
- **Intelligent Retry**: Selective retries for failed places.
- **Progress Tracking**: Real-time progress display and state saving.
- **Multiple Run Modes**: Test, production, unattended, etc.

#### Core Architecture

```python
class BatchGoogleMapsScraper:
    def __init__(self, script_path, output_base_dir, config):
        # Initialize configuration and paths
        
    def process_batch(self, places, **kwargs):
        # Main logic for batch processing
        
    def scrape_single_place(self, place_data):
        # Process a single place
        
    def save_progress(self, progress):
        # Save processing progress
```

#### Configuration Modes

1. **Default Mode** - Balanced performance and stability
   ```python
   DEFAULT_CONFIG = {
       'max_workers': 3,
       'max_retries': 3,
       'timeout': 300,
       'max_places': None
   }
   ```

2. **Test Mode** - For quick feature validation
   ```python
   TEST_CONFIG = {
       'max_workers': 1,
       'max_places': 5,
       'timeout': 60
   }
   ```

3. **Production Mode** - For large-scale processing
   ```python
   PRODUCTION_CONFIG = {
       'max_workers': 5,
       'max_places': 1000,
       'timeout': 600
   }
   ```

4. **Unattended Mode** - For long-term automatic operation
   ```python
   UNATTENDED_CONFIG = {
       'max_workers': 6,
       'max_places': 120,
       'continuous_mode': True,
       'auto_retry_failed': True
   }
   ```

#### Usage Example

```bash
# Basic batch processing
python batch_scraper.py input/places.jsonl

# Test mode
python batch_scraper.py input/places.jsonl --test

# Production mode
python batch_scraper.py input/places.jsonl --production \
    --max-workers 8 --max-places 2000

# Unattended mode
python batch_scraper.py input/places.jsonl --unattended

# Custom configuration
python batch_scraper.py input/places.jsonl \
    --max-workers 4 \
    --max-retries 5 \
    --timeout 600 \
    --max-places 500 \
    --resume \
    --retry-failed
```

#### Output File Structure

```
batch_output/
â”œâ”€â”€ parent_type/
â”‚   â””â”€â”€ sub_type/
â”‚       â”œâ”€â”€ place_id_BusinessName.json
â”‚       â””â”€â”€ place_id_BusinessName.csv
â”œâ”€â”€ progress.json          # Processing progress
â”œâ”€â”€ errors.jsonl          # Error log
â””â”€â”€ success.jsonl         # Success log
```

---

## Utility Modules

### process_urls.py - URL Pre-processor

**Functional Description**: Batch processes input files to add the `&hl=en` parameter to Google Maps URLs, forcing the browser to use the English interface. This is a critical pre-processing step to ensure the scraper can parse pages correctly.

#### Key Features
- **Force English**: Ensures all URLs point to the English version of Google Maps.
- **Idempotent**: Does not repeatedly add the parameter to URLs that already contain `&hl=en`.
- **Fault-Tolerant**: Skips malformed JSON lines in the file.

#### Usage
1.  **Configure Paths**: In the `process_urls.py` script, modify the `input_file` and `output_file` variables to match your file paths.
2.  **Execute Script**:
    ```bash
    python process_urls.py
    ```
3.  **Use Output**: Use the new file generated by the script (e.g., `..._en.jsonl`) as input for `batch_scraper.py`.

#### Core Logic
```python
def process_urls():
    with open(input_file, 'r', encoding='utf-8') as infile, \
         open(output_file, 'w', encoding='utf-8') as outfile:
        for line in infile:
            try:
                data = json.loads(line)
                # In the actual script, the key is 'Maps_url'. 
                # This is a representative example.
                if 'url' in data and data['url']:
                    if '&hl=en' not in data['url']:
                        data['url'] += '&hl=en'
                outfile.write(json.dumps(data, ensure_ascii=False) + '\n')
            except json.JSONDecodeError:
                print(f"Skipping malformed JSON line: {line.strip()}")
```

### check_progress.py - Progress Checker

**Functional Description**: Quickly check the progress and statistics of batch processing.

#### Key Functions

- **Progress Statistics**: Number of successful, failed, and skipped places.
- **Duplicate Detection**: Checks for duplicate processing of places.
- **Status Analysis**: Analyzes the distribution of processing statuses.

#### Usage

```bash
python check_progress.py
```

#### Example Output

```
=== Progress.json Statistics ===
successful count: 1250
failed count: 45
skipped count: 0
total processed: 1295

=== Statistics after Deduplication ===
unique successful: 1250
unique failed: 45
unique skipped: 0

=== Intersection Check ===
successful & failed: 0
successful & skipped: 0
failed & skipped: 0

=== Total ===
total unique processed IDs: 1295
```

#### Code Analysis

```python
# Read the progress file
with open('batch_output/progress.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

# Count quantities for each status
successful_count = len(data['successful'])
failed_count = len(data['failed'])
skipped_count = len(data['skipped'])

# Check for duplicates and intersections
successful_set = set(data['successful'])
failed_set = set(data['failed'])
intersection = successful_set & failed_set
```

### retry_failed_scrapes.py - Failure Retrier

**Functional Description**: Specifically retries previously failed places to improve data integrity.

#### Key Features

- **Intelligent Filtering**: Automatically identifies failed places.
- **Batch Retry**: Supports batch reprocessing of failed items.
- **Configuration Inheritance**: Inherits the original batch processing configuration.
- **Progress Merging**: Retry results are automatically merged into the main progress file.

#### Usage

```bash
# Basic retry
python retry_failed_scrapes.py

# Specify input file and configuration
python retry_failed_scrapes.py \
    --input input/places.jsonl \
    --max-workers 3 \
    --max-retries 5
```

#### Workflow

1. **Read Progress File**: Get the list of failed places from `progress.json`.
2. **Load Original Data**: Find the corresponding place information from the input file.
3. **Execute Retry**: Reprocess using `batch_scraper`.
4. **Update Progress**: Update the results in the progress file.

#### Core Logic

```python
def retry_failed_places():
    # 1. Read the list of failed places
    progress = load_progress()
    failed_place_ids = progress.get('failed', [])
    
    # 2. Load original place data
    original_places = load_input_data()
    failed_places = filter_failed_places(original_places, failed_place_ids)
    
    # 3. Reprocess
    scraper = BatchGoogleMapsScraper()
    scraper.process_batch(failed_places, retry_mode=True)
    
    # 4. Update progress
    update_progress(results)
```

### find_duplicates_simple.py - Duplicate Detection

**Functional Description**: Detects and handles duplicate items in the output data.

#### Key Functions

- **Duplicate Detection**: Detects duplicate data files based on `place_id`.
- **Statistical Report**: Generates a detailed report on duplicates.
- **Cleanup Suggestions**: Provides suggestions for cleaning up duplicate files.

#### Usage

```bash
python find_duplicates_simple.py
```

#### Detection Logic

```python
def find_duplicates():
    file_dict = {}
    duplicates = []
    
    # Iterate through all output files
    for file_path in glob("batch_output/**/*.json"):
        place_id = extract_place_id(file_path)
        
        if place_id in file_dict:
            duplicates.append({
                'place_id': place_id,
                'files': [file_dict[place_id], file_path]
            })
        else:
            file_dict[place_id] = file_path
    
    return duplicates
```

#### Output Report

```
=== Duplicate File Detection Report ===
Total files: 1295
Unique place_ids: 1290
Duplicate place_ids: 5

Duplicate Details:
place_id: ChIJ123...
  - batch_output/food_and_drink/restaurant/ChIJ123_RestaurantA.json
  - batch_output/food_and_drink/chinese_restaurant/ChIJ123_RestaurantA.json
```

### input_statistics.py - Input Data Statistics

**Functional Description**: Analyzes the distribution and statistical information of the input data.

#### Key Functions

- **Type Distribution**: Counts the number of each `parent_type` and `sub_type`.
- **Data Quality**: Checks for missing fields and format issues.
- **Geographic Distribution**: Analyzes the geographic distribution of places (if coordinates are available).

#### Usage

```bash
python input_statistics.py input/places.jsonl
```

#### Analysis Dimensions

1. **Total Data Statistics**
   ```python
   total_places = len(places)
   unique_place_ids = len(set(p['place_id'] for p in places))
   ```

2. **Type Distribution Statistics**
   ```python
   parent_type_dist = Counter(p['parent_type'] for p in places)
   sub_type_dist = Counter(p['sub_type'] for p in places)
   ```

3. **Data Quality Check**
   ```python
   missing_fields = {
       'place_id': sum(1 for p in places if not p.get('place_id')),
       'url': sum(1 for p in places if not p.get('url')),
       'place_name': sum(1 for p in places if not p.get('place_name'))
   }
   ```

#### Example Output

```
=== Input Data Statistics Report ===

Basic Statistics:
Total places: 15,847
Unique place_ids: 15,847
Duplicate place_ids: 0

Parent Type Distribution:
food_and_drink: 8,234 (52.0%)
shopping: 3,456 (21.8%)
entertainment_and_recreation: 2,123 (13.4%)
transportation: 1,234 (7.8%)
Other: 800 (5.0%)

Sub Type Distribution (Top 10):
restaurant: 3,456
store: 2,345
cafe: 1,234
park: 987
...

Data Quality Check:
Missing place_id: 0
Missing URL: 12
Missing place_name: 5
Invalid URL format: 3
```

---

## Configuration and Error Handling

### error_codes.md - Error Code Explanations

**Functional Description**: Standardized error code definitions and problem solutions.

#### Error Code Categories

1. **Browser-related Errors (1000-1099)**
   - `1001`: Browser initialization failed
   - `1002`: URL loading failed

2. **Data Extraction Errors (1100-1199)**
   - `1003`: Business info extraction failed
   - `1004`: Reviews button not found
   - `1005`: Review scrolling failed
   - `1006`: Review extraction failed
   - `1007`: Coordinate extraction failed

3. **File Operation Errors (1200-1299)**
   - `1008`: CSV save failed

4. **Network-related Errors (1300-1399)**
   - `1009`: Network timeout

5. **General Errors (1900-1999)**
   - `1010`: Element not found
   - `1999`: Unexpected error

#### Error Handling Strategy

```python
class ErrorCodes:
    SUCCESS = 0
    BROWSER_INIT_FAILED = 1001
    URL_LOAD_FAILED = 1002
    # ... other error codes
    
def handle_error(error_code, error_message):
    """Error handling and retry strategy"""
    if error_code in [1001, 1002]:
        # Browser-related error, restart browser
        return "restart_browser"
    elif error_code in [1009, 1010]:
        # Network or element issue, retryable
        return "retry"
    else:
        # Other errors, log and skip
        return "skip"
```

---

## ğŸ”§ Development and Extension

### Adding New Features

1. **Extend Data Extraction Fields**
   ```python
   # Add new extraction logic in main.py
   def extract_additional_info(driver):
       # New data extraction logic
       pass
   ```

2. **Custom Output Formats**
   ```python
   # Add a new output handler in batch_scraper.py
   def save_custom_format(data, output_path):
       # Custom format saving logic
       pass
   ```

3. **Integrate Database Storage**
   ```python
   # Add database connection and storage logic
   def save_to_database(data):
       # Database saving logic
       pass
   ```

### Performance Optimization Suggestions

1. **Memory Optimization**: Use generators to process large files.
2. **Concurrency Optimization**: Adjust the number of threads based on machine performance.
3. **Network Optimization**: Add request intervals and retry mechanisms.
4. **Storage Optimization**: Compress output files or use more efficient formats.

### Monitoring and Maintenance

1. **Log Monitoring**: Regularly check error logs.
2. **Performance Monitoring**: Track processing speed and success rate.
3. **Data Quality**: Periodically validate the integrity of the output data.
4. **Version Management**: Keep code and dependencies updated.

</rewritten_file>