#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google Mapsè¯„è®ºçˆ¬è™«
ä»Google Mapså•†æˆ·é¡µé¢æå–å•†æˆ·ä¿¡æ¯å’Œå®¢æˆ·è¯„è®º
"""

import os
import sys
import json
import time
import re
import logging
import argparse
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException

# ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šç¡®ä¿æ ‡å‡†è¾“å‡ºä½¿ç”¨UTF-8ç¼–ç 
if sys.platform.startswith('win'):
    # Windowsç³»ç»Ÿéœ€è¦ç‰¹æ®Šå¤„ç†
    import codecs
    if hasattr(sys.stdout, 'buffer'):
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
    if hasattr(sys.stderr, 'buffer'):
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

# è®¾ç½®ç¯å¢ƒå˜é‡ç¡®ä¿å­è¿›ç¨‹ä¹Ÿä½¿ç”¨UTF-8
os.environ['PYTHONIOENCODING'] = 'utf-8'

# é”™è¯¯ä»£ç å®šä¹‰
class ErrorCodes:
    SUCCESS = 0
    BROWSER_INIT_FAILED = 1001
    URL_LOAD_FAILED = 1002
    BUSINESS_INFO_EXTRACTION_FAILED = 1003
    REVIEWS_BUTTON_NOT_FOUND = 1004
    REVIEWS_SCROLL_FAILED = 1005
    REVIEWS_EXTRACTION_FAILED = 1006
    COORDINATES_EXTRACTION_FAILED = 1007
    CSV_SAVE_FAILED = 1008
    NETWORK_TIMEOUT = 1009
    ELEMENT_NOT_FOUND = 1010
    UNEXPECTED_ERROR = 1999

def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºJSONè¾“å‡ºæ¨¡å¼ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å°†æ‰€æœ‰æ—¥å¿—è¾“å‡ºåˆ°stderr
    # è¿™æ ·stdoutå°±åªæœ‰çº¯JSONæ•°æ®ï¼Œä¸ä¼šè¢«æ—¥å¿—ä¿¡æ¯æ±¡æŸ“
    json_mode = '--json-output' in sys.argv
    
    logger = logging.getLogger('google_maps_scraper')
    
    # æ¸…é™¤ä»»ä½•ç°æœ‰çš„handler
    logger.handlers.clear()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.setLevel(logging.INFO)
    
    # åˆ›å»ºformatter
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    
    # ğŸ”¥ åœ¨JSONæ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰æ—¥å¿—éƒ½è¾“å‡ºåˆ°stderrï¼Œå¦åˆ™è¾“å‡ºåˆ°stdout
    log_stream = sys.stderr if json_mode else sys.stdout
    handler = logging.StreamHandler(log_stream)
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    return logger

def safe_find_element(driver, by, value, timeout=10, required=False):
    """
    å®‰å…¨æŸ¥æ‰¾å…ƒç´ 
    
    Args:
        driver: WebDriverå®ä¾‹
        by: æŸ¥æ‰¾æ–¹å¼
        value: æŸ¥æ‰¾å€¼
        timeout: è¶…æ—¶æ—¶é—´
        required: æ˜¯å¦å¿…éœ€æ‰¾åˆ°
        
    Returns:
        elementæˆ–None
    """
    try:
        element = WebDriverWait(driver, timeout).until(
            EC.presence_of_element_located((by, value))
        )
        return element
    except TimeoutException:
        if required:
            raise
        return None

def safe_get_text(element, default=""):
    """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬"""
    try:
        return element.text.strip() if element else default
    except:
        return default

def safe_get_attribute(element, attr, default=""):
    """å®‰å…¨è·å–å…ƒç´ å±æ€§"""
    try:
        result = element.get_attribute(attr) if element else None
        return result.strip() if result else default
    except:
        return default

def scrape_single_url(url, output_dir="output", headless=True, max_retries=3):
    """
    çˆ¬å–å•ä¸ªURLçš„æ•°æ®
    è¿”å›ç»“æœå­—å…¸å’ŒçŠ¶æ€ä¿¡æ¯
    """
    logger = logging.getLogger(__name__)
    
    # æ»šåŠ¨å‚æ•°
    last_height = 0
    same_count = 0
    max_no_change = 3
    max_scrolls = 30
    coordinate_retries = 10

    result = {
        "success": False,
        "error_code": ErrorCodes.SUCCESS,
        "error_message": "",
        "business_info": {},
        "reviews_count": 0,
        "reviews": [],
        "csv_file": None,
        "url": url
    }

    driver = None
    
    for attempt in range(max_retries):
        try:
            logger.info(f"å°è¯•ç¬¬ {attempt + 1} æ¬¡çˆ¬å–: {url}")
            
            # è®¾ç½®Chromeé€‰é¡¹
            options = Options()
            if headless:
                options.add_argument("--headless")
            options.add_argument("--start-maximized")
            options.add_argument("--disable-blink-features=AutomationControlled")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            
            # ğŸ”¥ åœ¨JSONè¾“å‡ºæ¨¡å¼ä¸‹æ·»åŠ é¢å¤–çš„é™é»˜å‚æ•°
            json_mode = '--json-output' in sys.argv
            if json_mode:
                options.add_argument("--disable-logging")
                options.add_argument("--disable-gpu-logging")
                options.add_argument("--disable-chromium-logging")
                options.add_argument("--log-level=3")  # åªæ˜¾ç¤ºFATALçº§åˆ«
                options.add_argument("--silent")
                options.add_argument("--disable-background-timer-throttling")
                options.add_argument("--disable-renderer-backgrounding")
                options.add_argument("--disable-backgrounding-occluded-windows")
                options.add_argument("--disable-extensions-http-throttling")
                
                # ç¦ç”¨å„ç§æœåŠ¡å’ŒåŠŸèƒ½çš„æ—¥å¿—è¾“å‡º
                options.add_experimental_option('excludeSwitches', ['enable-logging'])
                options.add_experimental_option('useAutomationExtension', False)
                options.add_experimental_option("excludeSwitches", ["enable-automation"])
                
                # è®¾ç½®æ—¥å¿—é¦–é€‰é¡¹
                prefs = {
                    "profile.default_content_setting_values": {
                        "notifications": 2
                    }
                }
                options.add_experimental_option("prefs", prefs)

            # åˆ›å»º WebDriver å®ä¾‹
            try:
                driver = webdriver.Chrome(options=options)
                driver.set_page_load_timeout(30)
                logger.info("Chromeæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            except Exception as e:
                result["error_code"] = ErrorCodes.BROWSER_INIT_FAILED
                result["error_message"] = f"æ— æ³•å¯åŠ¨Chromeæµè§ˆå™¨: {str(e)}"
                logger.error(result["error_message"])
                if attempt == max_retries - 1:
                    return result
                continue

            # åŠ è½½é¡µé¢
            try:
                logger.info(f"æ­£åœ¨åŠ è½½é¡µé¢: {url}")
                driver.get(url)
                time.sleep(3)
            except Exception as e:
                result["error_code"] = ErrorCodes.URL_LOAD_FAILED
                result["error_message"] = f"é¡µé¢åŠ è½½å¤±è´¥: {str(e)}"
                logger.error(result["error_message"])
                if attempt == max_retries - 1:
                    return result
                continue

            info = {"url": url}
            
            # æå–å•†æˆ·åŸºæœ¬ä¿¡æ¯
            try:
                logger.info("æ­£åœ¨æå–å•†æˆ·åŸºæœ¬ä¿¡æ¯...")
                
                # å•†æˆ·åç§°
                name_elem = safe_find_element(driver, By.CSS_SELECTOR, 'h1.DUwDvf.lfPIob', timeout=15)
                info['name'] = safe_get_text(name_elem) or "æœªçŸ¥å•†æˆ·"
                
                # è¯„åˆ†
                rating_elem = safe_find_element(driver, By.CSS_SELECTOR, 'span.ceNzKf', timeout=10)
                info['rating'] = safe_get_attribute(rating_elem, 'aria-label') or "æ— è¯„åˆ†"
                
                # ç±»åˆ«
                category_elem = safe_find_element(driver, By.CSS_SELECTOR, 'button.DkEaL', timeout=10)
                info['category'] = safe_get_text(category_elem) or "æœªçŸ¥ç±»åˆ«"
                
                logger.info(f"å•†æˆ·ä¿¡æ¯æå–æˆåŠŸ: {info['name']}")
                
            except Exception as e:
                result["error_code"] = ErrorCodes.BUSINESS_INFO_EXTRACTION_FAILED
                result["error_message"] = f"å•†æˆ·ä¿¡æ¯æå–å¤±è´¥: {str(e)}"
                logger.warning(result["error_message"])
                info.update({'name': 'æœªçŸ¥å•†æˆ·', 'rating': 'æ— è¯„åˆ†', 'category': 'æœªçŸ¥ç±»åˆ«'})

            # ç‚¹å‡»Reviewsæ ‡ç­¾
            all_reviews = []
            try:
                logger.info("æ­£åœ¨æŸ¥æ‰¾ReviewsæŒ‰é’®...")
                
                # å°è¯•å¤šç§æ–¹å¼æ‰¾åˆ°ReviewsæŒ‰é’®
                reviews_selectors = [
                    "//div[@class='LRkQ2']//div[text()='Reviews']",
                    "//button[contains(text(), 'Reviews')]",
                    "//div[contains(text(), 'Reviews')]",
                    "//span[contains(text(), 'Reviews')]"
                ]
                
                reviews_element = None
                for selector in reviews_selectors:
                    elements = driver.find_elements(By.XPATH, selector)
                    if elements:
                        reviews_element = elements[0]
                        break
                
                if reviews_element:
                    driver.execute_script("arguments[0].click();", reviews_element)
                    time.sleep(3)
                    logger.info("ReviewsæŒ‰é’®ç‚¹å‡»æˆåŠŸ")
                    
                    # æŸ¥æ‰¾æ»šåŠ¨å®¹å™¨
                    try:
                        scrollable_div = safe_find_element(
                            driver, By.CSS_SELECTOR, 
                            'div.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde', 
                            timeout=10, required=True
                        )
                        
                        logger.info("å¼€å§‹æ»šåŠ¨åŠ è½½è¯„è®º...")
                        # æ»šåŠ¨åŠ è½½è¯„è®º
                        for scroll_count in range(max_scrolls):
                            current_height = driver.execute_script("return arguments[0].scrollHeight", scrollable_div)
                            
                            if current_height == last_height:
                                same_count += 1
                                if same_count >= max_no_change:
                                    logger.info(f"æ»šåŠ¨å®Œæˆï¼Œå…±æ»šåŠ¨ {scroll_count + 1} æ¬¡")
                                    break
                            else:
                                same_count = 0

                            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", scrollable_div)
                            last_height = current_height
                            time.sleep(1)
                        
                        # æå–è¯„è®º
                        logger.info("æ­£åœ¨æå–è¯„è®ºæ•°æ®...")
                        review_elements = driver.find_elements(By.CSS_SELECTOR, 'div.jftiEf.fontBodyMedium')
                        
                        for review in review_elements:
                            try:
                                # ç”¨æˆ·å
                                username = safe_get_attribute(review, "aria-label", "åŒ¿åç”¨æˆ·")
                                
                                # å†…å®¹
                                content_span = review.find_element(By.CSS_SELECTOR, 'span.wiI7pd')
                                content = safe_get_text(content_span)
                                
                                # æ˜Ÿçº§
                                rating_span = review.find_element(By.CSS_SELECTOR, 'span.kvMYJc')
                                rating = safe_get_attribute(rating_span, 'aria-label', "æ— è¯„åˆ†")
                                
                                # æ—¶é—´
                                time_element = review.find_element(By.CSS_SELECTOR, 'span.rsqaWe')
                                time_text = safe_get_text(time_element)
                                
                                # ç”¨æˆ·é™„åŠ ä¿¡æ¯
                                try:
                                    info_line = review.find_element(By.CSS_SELECTOR, 'div.RfnDt').text.strip()
                                except:
                                    info_line = ""

                                all_reviews.append({
                                    'user': username,
                                    'user_info': info_line,
                                    'rating': rating,
                                    'time': time_text,
                                    'content': content,
                                })
                            except Exception as e:
                                logger.warning(f"å•æ¡è¯„è®ºæå–å¤±è´¥: {str(e)}")
                                continue
                                
                        logger.info(f"æˆåŠŸæå– {len(all_reviews)} æ¡è¯„è®º")
                        
                    except Exception as e:
                        result["error_code"] = ErrorCodes.REVIEWS_SCROLL_FAILED
                        result["error_message"] = f"è¯„è®ºæ»šåŠ¨åŠ è½½å¤±è´¥: {str(e)}"
                        logger.warning(result["error_message"])
                        
                else:
                    result["error_code"] = ErrorCodes.REVIEWS_BUTTON_NOT_FOUND
                    result["error_message"] = "æœªæ‰¾åˆ°ReviewsæŒ‰é’®"
                    logger.warning(result["error_message"])
                    
            except Exception as e:
                result["error_code"] = ErrorCodes.REVIEWS_EXTRACTION_FAILED
                result["error_message"] = f"è¯„è®ºæå–è¿‡ç¨‹å¤±è´¥: {str(e)}"
                logger.warning(result["error_message"])

            # æå–åæ ‡
            lat, lon = None, None
            try:
                logger.info("æ­£åœ¨æå–åæ ‡ä¿¡æ¯...")
                
                for attempt_coord in range(coordinate_retries):
                    try:
                        current_url = driver.current_url
                        
                        # å°è¯•å¤šç§åæ ‡æå–æ–¹å¼
                        patterns = [
                            r'!3d([0-9\.\-]+)!4d([0-9\.\-]+)',
                            r'/@([0-9\.\-]+),([0-9\.\-]+)',
                            r'@([0-9\.\-]+),([0-9\.\-]+)',
                        ]
                        
                        for pattern in patterns:
                            match = re.search(pattern, current_url)
                            if match:
                                lat, lon = match.group(1), match.group(2)
                                logger.info(f"åæ ‡æå–æˆåŠŸ: {lat}, {lon}")
                                break
                        
                        if lat and lon:
                            break
                        else:
                            time.sleep(1)
                            
                    except Exception as e:
                        if attempt_coord == coordinate_retries - 1:
                            logger.warning(f"åæ ‡æå–å¤±è´¥: {str(e)}")
                        continue
                        
            except Exception as e:
                result["error_code"] = ErrorCodes.COORDINATES_EXTRACTION_FAILED
                result["error_message"] = f"åæ ‡æå–å¤±è´¥: {str(e)}"
                logger.warning(result["error_message"])

            info['lat'] = lat or ''
            info['lon'] = lon or ''
            
            # ä¿å­˜æ•°æ®
            try:
                if len(all_reviews) == 0:
                    df = pd.DataFrame([info])
                else:
                    df = pd.DataFrame(all_reviews)
                    for key, value in info.items():
                        df[key] = value
                
                # ä¿å­˜CSVæ–‡ä»¶
                if info['name'] and info['name'] != 'æœªçŸ¥å•†æˆ·':
                    # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
                    safe_name = re.sub(r'[<>:"/\\|?*]', '_', info['name'])
                    filename = f"{output_dir}/{safe_name}.csv"
                    df.to_csv(filename, index=False, encoding='utf-8-sig')
                    result["csv_file"] = filename
                    logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
                
            except Exception as e:
                result["error_code"] = ErrorCodes.CSV_SAVE_FAILED
                result["error_message"] = f"CSVæ–‡ä»¶ä¿å­˜å¤±è´¥: {str(e)}"
                logger.error(result["error_message"])

            # æˆåŠŸå®Œæˆ
            # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä¿ç•™åŸæœ‰çš„é”™è¯¯ä»£ç ï¼Œç‰¹åˆ«æ˜¯Reviewsç›¸å…³çš„é”™è¯¯ä»£ç 
            final_error_code = result.get("error_code", ErrorCodes.SUCCESS)
            final_error_message = result.get("error_message", "")
            
            result.update({
                "success": True,
                "error_code": final_error_code,  # ä¿ç•™åŸæœ‰é”™è¯¯ä»£ç 
                "error_message": final_error_message,  # ä¿ç•™åŸæœ‰é”™è¯¯ä¿¡æ¯
                "business_info": info,
                "reviews_count": len(all_reviews),
                "reviews": all_reviews
            })
            
            logger.info(f"çˆ¬å–æˆåŠŸå®Œæˆ: {info['name']}, {len(all_reviews)} æ¡è¯„è®º")
            return result
            
        except Exception as e:
            result["error_code"] = ErrorCodes.UNEXPECTED_ERROR
            result["error_message"] = f"æ„å¤–é”™è¯¯: {str(e)}"
            logger.error(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {result['error_message']}")
            
            if attempt < max_retries - 1:
                logger.info("ç­‰å¾…é‡è¯•...")
                time.sleep(5)
            
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
                driver = None

    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
    if not result["error_message"]:
        result["error_message"] = f"ç»è¿‡ {max_retries} æ¬¡é‡è¯•ä»ç„¶å¤±è´¥"
    
    logger.error(f"çˆ¬å–æœ€ç»ˆå¤±è´¥: {result['error_message']}")
    return result

def main():
    parser = argparse.ArgumentParser(description='Google Mapsè¯„è®ºçˆ¬è™«')
    parser.add_argument('--url', type=str, help='è¦çˆ¬å–çš„Google Maps URL')
    parser.add_argument('--urls-file', type=str, default='input/urls.txt', help='åŒ…å«URLåˆ—è¡¨çš„æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default='output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--headless', action='store_true', help='æ— ç•Œé¢æ¨¡å¼è¿è¡Œ')
    parser.add_argument('--json-output', action='store_true', help='ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ')
    parser.add_argument('--max-retries', type=int, default=3, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†æ—¥å¿—è¾“å‡º')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger = setup_logging()
    if args.verbose:
        logger.setLevel(logging.DEBUG)
    
    urls = []
    
    # å¦‚æœæŒ‡å®šäº†å•ä¸ªURL
    if args.url:
        urls = [args.url]
    else:
        # ä»æ–‡ä»¶è¯»å–URLåˆ—è¡¨
        try:
            with open(args.urls_file, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            error_result = {
                "success": False,
                "error_code": ErrorCodes.ELEMENT_NOT_FOUND,
                "error_message": f"æ–‡ä»¶ {args.urls_file} ä¸å­˜åœ¨"
            }
            if args.json_output:
                print(json.dumps([error_result], ensure_ascii=False))
            else:
                print(f"é”™è¯¯: {error_result['error_message']}", file=sys.stderr)
            sys.exit(1)
    
    if not urls:
        error_result = {
            "success": False,
            "error_code": ErrorCodes.ELEMENT_NOT_FOUND,
            "error_message": "æ²¡æœ‰æ‰¾åˆ°è¦å¤„ç†çš„URL"
        }
        if args.json_output:
            print(json.dumps([error_result], ensure_ascii=False))
        else:
            print(f"é”™è¯¯: {error_result['error_message']}", file=sys.stderr)
        sys.exit(1)
    
    results = []
    
    for i, url in enumerate(urls, 1):
        if not args.json_output:
            print(f"[{i}/{len(urls)}] æ­£åœ¨å¤„ç†: {url}")
        
        result = scrape_single_url(url, args.output_dir, args.headless, args.max_retries)
        
        if args.json_output:
            results.append(result)
        else:
            if result["success"]:
                business_name = result['business_info'].get('name', 'æœªçŸ¥')
                reviews_count = result['reviews_count']
                print(f"âœ“ {business_name}: æˆåŠŸæå– {reviews_count} æ¡è¯„è®º")
            else:
                print(f"âœ— å¤±è´¥ (é”™è¯¯ä»£ç : {result['error_code']}): {result['error_message']}")
    
    if args.json_output:
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šåœ¨è¾“å‡ºJSONå‰ï¼Œç¡®ä¿æ ‡å‡†è¾“å‡ºæ­£ç¡®é…ç½®
        import locale
        
        # å°è¯•è®¾ç½®localeä¸ºUTF-8
        try:
            if sys.platform.startswith('win'):
                locale.setlocale(locale.LC_ALL, '')
        except:
            pass
            
        # å¼ºåˆ¶åˆ·æ–°æ ‡å‡†è¾“å‡ºç¼“å†²åŒº
        sys.stdout.flush()
        
        # è¾“å‡ºJSONï¼Œæ˜ç¡®æŒ‡å®šç¼–ç é€‰é¡¹
        json_output = json.dumps(results, ensure_ascii=False, indent=2)
        
        # åœ¨Windowsä¸‹ï¼Œç¡®ä¿æ­£ç¡®è¾“å‡º
        if sys.platform.startswith('win'):
            try:
                # å°è¯•ç›´æ¥è¾“å‡ºå­—èŠ‚
                sys.stdout.buffer.write(json_output.encode('utf-8'))
                sys.stdout.buffer.write(b'\n')
                sys.stdout.buffer.flush()
            except (AttributeError, UnicodeEncodeError):
                # å¤‡ç”¨æ–¹æ¡ˆï¼šæ™®é€šè¾“å‡º
                print(json_output)
        else:
            print(json_output)
    else:
        success_count = sum(1 for r in results if r["success"])
        print(f"\nå¤„ç†å®Œæˆï¼æˆåŠŸ: {success_count}/{len(urls)}")
        
        # æ˜¾ç¤ºå¤±è´¥çš„URLå’Œé”™è¯¯
        failed_results = [r for r in results if not r["success"]]
        if failed_results:
            print("\nå¤±è´¥çš„URL:")
            for result in failed_results:
                print(f"  {result['url']} - é”™è¯¯ä»£ç : {result['error_code']} - {result['error_message']}")

if __name__ == "__main__":
    main()